{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.rcParams['font.family'] = 'monospace'\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Conv1D, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv1D, BatchNormalization, \n",
    "    Bidirectional, LSTM, Dropout, Dense,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKSPACE = os.path.abspath(os.path.join(os.getcwd()))\n",
    "DATA_FOLDER = os.path.join(WORKSPACE, 'productos/test/precipitation-forecast-co/data')\n",
    "MODELS_FOLDER = os.path.join(WORKSPACE, 'productos/test/precipitation-forecast-co/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "processing_data = os.path.join(DATA_FOLDER, 'processing/processing_month.parquet')\n",
    "df = pd.read_parquet(processing_data)\n",
    "\n",
    "# Expand IMFs and VMDs\n",
    "imf_n=df['IMFs'].str.len().max()\n",
    "imf_c=[f'imf{i+1}' for i in range(imf_n)]\n",
    "vmd_n=df['VMDs'].str.len().max()\n",
    "vmd_c=[f'vmd{i+1}' for i in range(vmd_n)]\n",
    "df=pd.concat([df,pd.DataFrame(df['IMFs'].tolist(),columns=imf_c),pd.DataFrame(df['VMDs'].tolist(),columns=vmd_c)],axis=1)\n",
    "df.drop(columns=['IMFs','VMDs'],inplace=True)\n",
    "\n",
    "def compute_sample_weights(y_data):\n",
    "    \"\"\"\n",
    "    Compute sample weights based on deviation from mean.\n",
    "    Samples farther from mean get higher weights.\n",
    "    \"\"\"\n",
    "    mean_value = np.mean(y_data)\n",
    "    # Calculate absolute deviation from mean\n",
    "    deviations = np.abs(y_data - mean_value)\n",
    "    # Normalize to get weights in a reasonable range (0.5 to 2.0)\n",
    "    # Add small epsilon to avoid division by zero\n",
    "    max_dev = np.max(deviations) + 1e-10\n",
    "    # Scale deviations to range [0, 1] and then to range [0.5, 2.0]\n",
    "    weights = 0.5 + 1.5 * (deviations / max_dev)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, context_length, feature):\n",
    "    \"\"\"\n",
    "    Prepares training and testing data from a column of normalized arrays.\n",
    "    \"\"\"\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "\n",
    "    for window in df[feature]:\n",
    "        if window is None:\n",
    "            continue\n",
    "        X, y = [], []\n",
    "        for i in range(len(window) - context_length):\n",
    "            X.append(window[i:i+context_length])\n",
    "            y.append(window[i+context_length])\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train.append(X[:split_idx])\n",
    "        X_test.append(X[split_idx:])\n",
    "        y_train.append(y[:split_idx])\n",
    "        y_test.append(y[split_idx:])\n",
    "\n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    X_test = np.concatenate(X_test, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "    y_test = np.concatenate(y_test, axis=0)\n",
    "\n",
    "    X_train = X_train.reshape(-1, context_length, 1)\n",
    "    X_test = X_test.reshape(-1, context_length, 1)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train: (209570, 32), y_train: (209570, 1)\n",
      "Forma de X_test: (54055, 32), y_test: (54055, 1)\n",
      "[0]\ttest-rmse:0.27902\n",
      "[1]\ttest-rmse:0.27351\n",
      "[2]\ttest-rmse:0.26821\n",
      "[3]\ttest-rmse:0.26307\n",
      "[4]\ttest-rmse:0.25812\n",
      "[5]\ttest-rmse:0.25340\n",
      "[6]\ttest-rmse:0.24898\n",
      "[7]\ttest-rmse:0.24463\n",
      "[8]\ttest-rmse:0.24048\n",
      "[9]\ttest-rmse:0.23647\n",
      "[10]\ttest-rmse:0.23266\n",
      "[11]\ttest-rmse:0.22898\n",
      "[12]\ttest-rmse:0.22554\n",
      "[13]\ttest-rmse:0.22217\n",
      "[14]\ttest-rmse:0.21896\n",
      "[15]\ttest-rmse:0.21595\n",
      "[16]\ttest-rmse:0.21306\n",
      "[17]\ttest-rmse:0.21029\n",
      "[18]\ttest-rmse:0.20760\n",
      "[19]\ttest-rmse:0.20503\n",
      "[20]\ttest-rmse:0.20263\n",
      "[21]\ttest-rmse:0.20032\n",
      "[22]\ttest-rmse:0.19808\n",
      "[23]\ttest-rmse:0.19604\n",
      "[24]\ttest-rmse:0.19402\n",
      "[25]\ttest-rmse:0.19208\n",
      "[26]\ttest-rmse:0.19029\n",
      "[27]\ttest-rmse:0.18855\n",
      "[28]\ttest-rmse:0.18688\n",
      "[29]\ttest-rmse:0.18528\n",
      "[30]\ttest-rmse:0.18377\n",
      "[31]\ttest-rmse:0.18236\n",
      "[32]\ttest-rmse:0.18102\n",
      "[33]\ttest-rmse:0.17972\n",
      "[34]\ttest-rmse:0.17845\n",
      "[35]\ttest-rmse:0.17728\n",
      "[36]\ttest-rmse:0.17620\n",
      "[37]\ttest-rmse:0.17514\n",
      "[38]\ttest-rmse:0.17413\n",
      "[39]\ttest-rmse:0.17320\n",
      "[40]\ttest-rmse:0.17230\n",
      "[41]\ttest-rmse:0.17145\n",
      "[42]\ttest-rmse:0.17062\n",
      "[43]\ttest-rmse:0.16982\n",
      "[44]\ttest-rmse:0.16909\n",
      "[45]\ttest-rmse:0.16839\n",
      "[46]\ttest-rmse:0.16775\n",
      "[47]\ttest-rmse:0.16711\n",
      "[48]\ttest-rmse:0.16650\n",
      "[49]\ttest-rmse:0.16591\n",
      "[50]\ttest-rmse:0.16538\n",
      "[51]\ttest-rmse:0.16487\n",
      "[52]\ttest-rmse:0.16439\n",
      "[53]\ttest-rmse:0.16392\n",
      "[54]\ttest-rmse:0.16348\n",
      "[55]\ttest-rmse:0.16305\n",
      "[56]\ttest-rmse:0.16265\n",
      "[57]\ttest-rmse:0.16227\n",
      "[58]\ttest-rmse:0.16193\n",
      "[59]\ttest-rmse:0.16159\n",
      "[60]\ttest-rmse:0.16127\n",
      "[61]\ttest-rmse:0.16096\n",
      "[62]\ttest-rmse:0.16067\n",
      "[63]\ttest-rmse:0.16039\n",
      "[64]\ttest-rmse:0.16013\n",
      "[65]\ttest-rmse:0.15987\n",
      "[66]\ttest-rmse:0.15964\n",
      "[67]\ttest-rmse:0.15942\n",
      "[68]\ttest-rmse:0.15921\n",
      "[69]\ttest-rmse:0.15903\n",
      "[70]\ttest-rmse:0.15885\n",
      "[71]\ttest-rmse:0.15865\n",
      "[72]\ttest-rmse:0.15848\n",
      "[73]\ttest-rmse:0.15832\n",
      "[74]\ttest-rmse:0.15817\n",
      "[75]\ttest-rmse:0.15803\n",
      "[76]\ttest-rmse:0.15789\n",
      "[77]\ttest-rmse:0.15774\n",
      "[78]\ttest-rmse:0.15761\n",
      "[79]\ttest-rmse:0.15748\n",
      "[80]\ttest-rmse:0.15737\n",
      "[81]\ttest-rmse:0.15726\n",
      "[82]\ttest-rmse:0.15716\n",
      "[83]\ttest-rmse:0.15706\n",
      "[84]\ttest-rmse:0.15696\n",
      "[85]\ttest-rmse:0.15687\n",
      "[86]\ttest-rmse:0.15679\n",
      "[87]\ttest-rmse:0.15672\n",
      "[88]\ttest-rmse:0.15665\n",
      "[89]\ttest-rmse:0.15656\n",
      "[90]\ttest-rmse:0.15647\n",
      "[91]\ttest-rmse:0.15638\n",
      "[92]\ttest-rmse:0.15631\n",
      "[93]\ttest-rmse:0.15623\n",
      "[94]\ttest-rmse:0.15616\n",
      "[95]\ttest-rmse:0.15609\n",
      "[96]\ttest-rmse:0.15601\n",
      "[97]\ttest-rmse:0.15594\n",
      "[98]\ttest-rmse:0.15589\n",
      "[99]\ttest-rmse:0.15584\n",
      "[100]\ttest-rmse:0.15578\n",
      "[101]\ttest-rmse:0.15572\n",
      "[102]\ttest-rmse:0.15566\n",
      "[103]\ttest-rmse:0.15560\n",
      "[104]\ttest-rmse:0.15557\n",
      "[105]\ttest-rmse:0.15552\n",
      "[106]\ttest-rmse:0.15548\n",
      "[107]\ttest-rmse:0.15543\n",
      "[108]\ttest-rmse:0.15539\n",
      "[109]\ttest-rmse:0.15535\n",
      "[110]\ttest-rmse:0.15531\n",
      "[111]\ttest-rmse:0.15527\n",
      "[112]\ttest-rmse:0.15524\n",
      "[113]\ttest-rmse:0.15519\n",
      "[114]\ttest-rmse:0.15516\n",
      "[115]\ttest-rmse:0.15512\n",
      "[116]\ttest-rmse:0.15509\n",
      "[117]\ttest-rmse:0.15506\n",
      "[118]\ttest-rmse:0.15504\n",
      "[119]\ttest-rmse:0.15499\n",
      "[120]\ttest-rmse:0.15496\n",
      "[121]\ttest-rmse:0.15492\n",
      "[122]\ttest-rmse:0.15490\n",
      "[123]\ttest-rmse:0.15486\n",
      "[124]\ttest-rmse:0.15484\n",
      "[125]\ttest-rmse:0.15480\n",
      "[126]\ttest-rmse:0.15478\n",
      "[127]\ttest-rmse:0.15475\n",
      "[128]\ttest-rmse:0.15472\n",
      "[129]\ttest-rmse:0.15470\n",
      "[130]\ttest-rmse:0.15467\n",
      "[131]\ttest-rmse:0.15464\n",
      "[132]\ttest-rmse:0.15462\n",
      "[133]\ttest-rmse:0.15460\n",
      "[134]\ttest-rmse:0.15458\n",
      "[135]\ttest-rmse:0.15456\n",
      "[136]\ttest-rmse:0.15455\n",
      "[137]\ttest-rmse:0.15453\n",
      "[138]\ttest-rmse:0.15451\n",
      "[139]\ttest-rmse:0.15449\n",
      "[140]\ttest-rmse:0.15447\n",
      "[141]\ttest-rmse:0.15445\n",
      "[142]\ttest-rmse:0.15443\n",
      "[143]\ttest-rmse:0.15440\n",
      "[144]\ttest-rmse:0.15439\n",
      "[145]\ttest-rmse:0.15438\n",
      "[146]\ttest-rmse:0.15436\n",
      "[147]\ttest-rmse:0.15434\n",
      "[148]\ttest-rmse:0.15433\n",
      "[149]\ttest-rmse:0.15430\n",
      "[150]\ttest-rmse:0.15429\n",
      "[151]\ttest-rmse:0.15427\n",
      "[152]\ttest-rmse:0.15425\n",
      "[153]\ttest-rmse:0.15424\n",
      "[154]\ttest-rmse:0.15423\n",
      "[155]\ttest-rmse:0.15421\n",
      "[156]\ttest-rmse:0.15419\n",
      "[157]\ttest-rmse:0.15417\n",
      "[158]\ttest-rmse:0.15416\n",
      "[159]\ttest-rmse:0.15414\n",
      "[160]\ttest-rmse:0.15412\n",
      "[161]\ttest-rmse:0.15410\n",
      "[162]\ttest-rmse:0.15409\n",
      "[163]\ttest-rmse:0.15407\n",
      "[164]\ttest-rmse:0.15405\n",
      "[165]\ttest-rmse:0.15404\n",
      "[166]\ttest-rmse:0.15401\n",
      "[167]\ttest-rmse:0.15400\n",
      "[168]\ttest-rmse:0.15399\n",
      "[169]\ttest-rmse:0.15397\n",
      "[170]\ttest-rmse:0.15396\n",
      "[171]\ttest-rmse:0.15395\n",
      "[172]\ttest-rmse:0.15394\n",
      "[173]\ttest-rmse:0.15392\n",
      "[174]\ttest-rmse:0.15391\n",
      "[175]\ttest-rmse:0.15389\n",
      "[176]\ttest-rmse:0.15387\n",
      "[177]\ttest-rmse:0.15386\n",
      "[178]\ttest-rmse:0.15384\n",
      "[179]\ttest-rmse:0.15382\n",
      "[180]\ttest-rmse:0.15381\n",
      "[181]\ttest-rmse:0.15379\n",
      "[182]\ttest-rmse:0.15378\n",
      "[183]\ttest-rmse:0.15377\n",
      "[184]\ttest-rmse:0.15376\n",
      "[185]\ttest-rmse:0.15375\n",
      "[186]\ttest-rmse:0.15374\n",
      "[187]\ttest-rmse:0.15373\n",
      "[188]\ttest-rmse:0.15372\n",
      "[189]\ttest-rmse:0.15371\n",
      "[190]\ttest-rmse:0.15370\n",
      "[191]\ttest-rmse:0.15369\n",
      "[192]\ttest-rmse:0.15369\n",
      "[193]\ttest-rmse:0.15368\n",
      "[194]\ttest-rmse:0.15368\n",
      "[195]\ttest-rmse:0.15366\n",
      "[196]\ttest-rmse:0.15366\n",
      "[197]\ttest-rmse:0.15364\n",
      "[198]\ttest-rmse:0.15364\n",
      "[199]\ttest-rmse:0.15362\n",
      "[200]\ttest-rmse:0.15361\n",
      "[201]\ttest-rmse:0.15359\n",
      "[202]\ttest-rmse:0.15359\n",
      "[203]\ttest-rmse:0.15358\n",
      "[204]\ttest-rmse:0.15357\n",
      "[205]\ttest-rmse:0.15356\n",
      "[206]\ttest-rmse:0.15356\n",
      "[207]\ttest-rmse:0.15354\n",
      "[208]\ttest-rmse:0.15353\n",
      "[209]\ttest-rmse:0.15353\n",
      "[210]\ttest-rmse:0.15352\n",
      "[211]\ttest-rmse:0.15351\n",
      "[212]\ttest-rmse:0.15351\n",
      "[213]\ttest-rmse:0.15349\n",
      "[214]\ttest-rmse:0.15348\n",
      "[215]\ttest-rmse:0.15348\n",
      "[216]\ttest-rmse:0.15347\n",
      "[217]\ttest-rmse:0.15346\n",
      "[218]\ttest-rmse:0.15345\n",
      "[219]\ttest-rmse:0.15344\n",
      "[220]\ttest-rmse:0.15343\n",
      "[221]\ttest-rmse:0.15342\n",
      "[222]\ttest-rmse:0.15341\n",
      "[223]\ttest-rmse:0.15340\n",
      "[224]\ttest-rmse:0.15340\n",
      "[225]\ttest-rmse:0.15339\n",
      "[226]\ttest-rmse:0.15338\n",
      "[227]\ttest-rmse:0.15338\n",
      "[228]\ttest-rmse:0.15337\n",
      "[229]\ttest-rmse:0.15337\n",
      "[230]\ttest-rmse:0.15336\n",
      "[231]\ttest-rmse:0.15335\n",
      "[232]\ttest-rmse:0.15335\n",
      "[233]\ttest-rmse:0.15334\n",
      "[234]\ttest-rmse:0.15332\n",
      "[235]\ttest-rmse:0.15331\n",
      "[236]\ttest-rmse:0.15330\n",
      "[237]\ttest-rmse:0.15330\n",
      "[238]\ttest-rmse:0.15329\n",
      "[239]\ttest-rmse:0.15328\n",
      "[240]\ttest-rmse:0.15327\n",
      "[241]\ttest-rmse:0.15326\n",
      "[242]\ttest-rmse:0.15325\n",
      "[243]\ttest-rmse:0.15324\n",
      "[244]\ttest-rmse:0.15323\n",
      "[245]\ttest-rmse:0.15322\n",
      "[246]\ttest-rmse:0.15321\n",
      "[247]\ttest-rmse:0.15321\n",
      "[248]\ttest-rmse:0.15321\n",
      "[249]\ttest-rmse:0.15321\n",
      "[250]\ttest-rmse:0.15320\n",
      "[251]\ttest-rmse:0.15320\n",
      "[252]\ttest-rmse:0.15319\n",
      "[253]\ttest-rmse:0.15317\n",
      "[254]\ttest-rmse:0.15316\n",
      "[255]\ttest-rmse:0.15315\n",
      "[256]\ttest-rmse:0.15314\n",
      "[257]\ttest-rmse:0.15313\n",
      "[258]\ttest-rmse:0.15313\n",
      "[259]\ttest-rmse:0.15312\n",
      "[260]\ttest-rmse:0.15311\n",
      "[261]\ttest-rmse:0.15310\n",
      "[262]\ttest-rmse:0.15309\n",
      "[263]\ttest-rmse:0.15308\n",
      "[264]\ttest-rmse:0.15308\n",
      "[265]\ttest-rmse:0.15307\n",
      "[266]\ttest-rmse:0.15306\n",
      "[267]\ttest-rmse:0.15306\n",
      "[268]\ttest-rmse:0.15306\n",
      "[269]\ttest-rmse:0.15305\n",
      "[270]\ttest-rmse:0.15304\n",
      "[271]\ttest-rmse:0.15303\n",
      "[272]\ttest-rmse:0.15302\n",
      "[273]\ttest-rmse:0.15301\n",
      "[274]\ttest-rmse:0.15300\n",
      "[275]\ttest-rmse:0.15299\n",
      "[276]\ttest-rmse:0.15299\n",
      "[277]\ttest-rmse:0.15298\n",
      "[278]\ttest-rmse:0.15297\n",
      "[279]\ttest-rmse:0.15296\n",
      "[280]\ttest-rmse:0.15296\n",
      "[281]\ttest-rmse:0.15295\n",
      "[282]\ttest-rmse:0.15294\n",
      "[283]\ttest-rmse:0.15294\n",
      "[284]\ttest-rmse:0.15293\n",
      "[285]\ttest-rmse:0.15292\n",
      "[286]\ttest-rmse:0.15291\n",
      "[287]\ttest-rmse:0.15289\n",
      "[288]\ttest-rmse:0.15288\n",
      "[289]\ttest-rmse:0.15287\n",
      "[290]\ttest-rmse:0.15287\n",
      "[291]\ttest-rmse:0.15285\n",
      "[292]\ttest-rmse:0.15284\n",
      "[293]\ttest-rmse:0.15285\n",
      "[294]\ttest-rmse:0.15284\n",
      "[295]\ttest-rmse:0.15283\n",
      "[296]\ttest-rmse:0.15283\n",
      "[297]\ttest-rmse:0.15283\n",
      "[298]\ttest-rmse:0.15282\n",
      "[299]\ttest-rmse:0.15281\n",
      "[300]\ttest-rmse:0.15281\n",
      "[301]\ttest-rmse:0.15280\n",
      "[302]\ttest-rmse:0.15279\n",
      "[303]\ttest-rmse:0.15279\n",
      "[304]\ttest-rmse:0.15278\n",
      "[305]\ttest-rmse:0.15277\n",
      "[306]\ttest-rmse:0.15276\n",
      "[307]\ttest-rmse:0.15276\n",
      "[308]\ttest-rmse:0.15275\n",
      "[309]\ttest-rmse:0.15274\n",
      "[310]\ttest-rmse:0.15273\n",
      "[311]\ttest-rmse:0.15271\n",
      "[312]\ttest-rmse:0.15271\n",
      "[313]\ttest-rmse:0.15270\n",
      "[314]\ttest-rmse:0.15269\n",
      "[315]\ttest-rmse:0.15268\n",
      "[316]\ttest-rmse:0.15267\n",
      "[317]\ttest-rmse:0.15267\n",
      "[318]\ttest-rmse:0.15266\n",
      "[319]\ttest-rmse:0.15266\n",
      "[320]\ttest-rmse:0.15265\n",
      "[321]\ttest-rmse:0.15265\n",
      "[322]\ttest-rmse:0.15264\n",
      "[323]\ttest-rmse:0.15263\n",
      "[324]\ttest-rmse:0.15263\n",
      "[325]\ttest-rmse:0.15262\n",
      "[326]\ttest-rmse:0.15262\n",
      "[327]\ttest-rmse:0.15261\n",
      "[328]\ttest-rmse:0.15261\n",
      "[329]\ttest-rmse:0.15260\n",
      "[330]\ttest-rmse:0.15260\n",
      "[331]\ttest-rmse:0.15258\n",
      "[332]\ttest-rmse:0.15258\n",
      "[333]\ttest-rmse:0.15257\n",
      "[334]\ttest-rmse:0.15256\n",
      "[335]\ttest-rmse:0.15254\n",
      "[336]\ttest-rmse:0.15253\n",
      "[337]\ttest-rmse:0.15252\n",
      "[338]\ttest-rmse:0.15251\n",
      "[339]\ttest-rmse:0.15251\n",
      "[340]\ttest-rmse:0.15250\n",
      "[341]\ttest-rmse:0.15249\n",
      "[342]\ttest-rmse:0.15248\n",
      "[343]\ttest-rmse:0.15248\n",
      "[344]\ttest-rmse:0.15246\n",
      "[345]\ttest-rmse:0.15245\n",
      "[346]\ttest-rmse:0.15244\n",
      "[347]\ttest-rmse:0.15245\n",
      "[348]\ttest-rmse:0.15244\n",
      "[349]\ttest-rmse:0.15243\n",
      "[350]\ttest-rmse:0.15243\n",
      "[351]\ttest-rmse:0.15242\n",
      "[352]\ttest-rmse:0.15240\n",
      "[353]\ttest-rmse:0.15238\n",
      "[354]\ttest-rmse:0.15239\n",
      "[355]\ttest-rmse:0.15238\n",
      "[356]\ttest-rmse:0.15237\n",
      "[357]\ttest-rmse:0.15236\n",
      "[358]\ttest-rmse:0.15236\n",
      "[359]\ttest-rmse:0.15235\n",
      "[360]\ttest-rmse:0.15234\n",
      "[361]\ttest-rmse:0.15234\n",
      "[362]\ttest-rmse:0.15233\n",
      "[363]\ttest-rmse:0.15232\n",
      "[364]\ttest-rmse:0.15231\n",
      "[365]\ttest-rmse:0.15231\n",
      "[366]\ttest-rmse:0.15231\n",
      "[367]\ttest-rmse:0.15231\n",
      "[368]\ttest-rmse:0.15230\n",
      "[369]\ttest-rmse:0.15230\n",
      "[370]\ttest-rmse:0.15229\n",
      "[371]\ttest-rmse:0.15229\n",
      "[372]\ttest-rmse:0.15228\n",
      "[373]\ttest-rmse:0.15227\n",
      "[374]\ttest-rmse:0.15226\n",
      "[375]\ttest-rmse:0.15226\n",
      "[376]\ttest-rmse:0.15225\n",
      "[377]\ttest-rmse:0.15225\n",
      "[378]\ttest-rmse:0.15224\n",
      "[379]\ttest-rmse:0.15224\n",
      "[380]\ttest-rmse:0.15223\n",
      "[381]\ttest-rmse:0.15222\n",
      "[382]\ttest-rmse:0.15222\n",
      "[383]\ttest-rmse:0.15220\n",
      "[384]\ttest-rmse:0.15219\n",
      "[385]\ttest-rmse:0.15218\n",
      "[386]\ttest-rmse:0.15218\n",
      "[387]\ttest-rmse:0.15218\n",
      "[388]\ttest-rmse:0.15216\n",
      "[389]\ttest-rmse:0.15216\n",
      "[390]\ttest-rmse:0.15216\n",
      "[391]\ttest-rmse:0.15215\n",
      "[392]\ttest-rmse:0.15215\n",
      "[393]\ttest-rmse:0.15214\n",
      "[394]\ttest-rmse:0.15213\n",
      "[395]\ttest-rmse:0.15213\n",
      "[396]\ttest-rmse:0.15212\n",
      "[397]\ttest-rmse:0.15211\n",
      "[398]\ttest-rmse:0.15211\n",
      "[399]\ttest-rmse:0.15210\n",
      "[400]\ttest-rmse:0.15209\n",
      "[401]\ttest-rmse:0.15209\n",
      "[402]\ttest-rmse:0.15208\n",
      "[403]\ttest-rmse:0.15207\n",
      "[404]\ttest-rmse:0.15207\n",
      "[405]\ttest-rmse:0.15207\n",
      "[406]\ttest-rmse:0.15207\n",
      "[407]\ttest-rmse:0.15206\n",
      "[408]\ttest-rmse:0.15205\n",
      "[409]\ttest-rmse:0.15204\n",
      "[410]\ttest-rmse:0.15204\n",
      "[411]\ttest-rmse:0.15204\n",
      "[412]\ttest-rmse:0.15203\n",
      "[413]\ttest-rmse:0.15201\n",
      "[414]\ttest-rmse:0.15200\n",
      "[415]\ttest-rmse:0.15200\n",
      "[416]\ttest-rmse:0.15200\n",
      "[417]\ttest-rmse:0.15199\n",
      "[418]\ttest-rmse:0.15198\n",
      "[419]\ttest-rmse:0.15197\n",
      "[420]\ttest-rmse:0.15196\n",
      "[421]\ttest-rmse:0.15195\n",
      "[422]\ttest-rmse:0.15194\n",
      "[423]\ttest-rmse:0.15194\n",
      "[424]\ttest-rmse:0.15193\n",
      "[425]\ttest-rmse:0.15193\n",
      "[426]\ttest-rmse:0.15191\n",
      "[427]\ttest-rmse:0.15191\n",
      "[428]\ttest-rmse:0.15191\n",
      "[429]\ttest-rmse:0.15190\n",
      "[430]\ttest-rmse:0.15190\n",
      "[431]\ttest-rmse:0.15189\n",
      "[432]\ttest-rmse:0.15188\n",
      "[433]\ttest-rmse:0.15188\n",
      "[434]\ttest-rmse:0.15187\n",
      "[435]\ttest-rmse:0.15186\n",
      "[436]\ttest-rmse:0.15186\n",
      "[437]\ttest-rmse:0.15185\n",
      "[438]\ttest-rmse:0.15185\n",
      "[439]\ttest-rmse:0.15184\n",
      "[440]\ttest-rmse:0.15184\n",
      "[441]\ttest-rmse:0.15183\n",
      "[442]\ttest-rmse:0.15183\n",
      "[443]\ttest-rmse:0.15182\n",
      "[444]\ttest-rmse:0.15181\n",
      "[445]\ttest-rmse:0.15180\n",
      "[446]\ttest-rmse:0.15179\n",
      "[447]\ttest-rmse:0.15179\n",
      "[448]\ttest-rmse:0.15177\n",
      "[449]\ttest-rmse:0.15177\n",
      "[450]\ttest-rmse:0.15177\n",
      "[451]\ttest-rmse:0.15176\n",
      "[452]\ttest-rmse:0.15175\n",
      "[453]\ttest-rmse:0.15174\n",
      "[454]\ttest-rmse:0.15174\n",
      "[455]\ttest-rmse:0.15174\n",
      "[456]\ttest-rmse:0.15174\n",
      "[457]\ttest-rmse:0.15173\n",
      "[458]\ttest-rmse:0.15173\n",
      "[459]\ttest-rmse:0.15172\n",
      "[460]\ttest-rmse:0.15171\n",
      "[461]\ttest-rmse:0.15171\n",
      "[462]\ttest-rmse:0.15170\n",
      "[463]\ttest-rmse:0.15170\n",
      "[464]\ttest-rmse:0.15169\n",
      "[465]\ttest-rmse:0.15169\n",
      "[466]\ttest-rmse:0.15169\n",
      "[467]\ttest-rmse:0.15168\n",
      "[468]\ttest-rmse:0.15167\n",
      "[469]\ttest-rmse:0.15167\n",
      "[470]\ttest-rmse:0.15167\n",
      "[471]\ttest-rmse:0.15166\n",
      "[472]\ttest-rmse:0.15166\n",
      "[473]\ttest-rmse:0.15165\n",
      "[474]\ttest-rmse:0.15164\n",
      "[475]\ttest-rmse:0.15164\n",
      "[476]\ttest-rmse:0.15164\n",
      "[477]\ttest-rmse:0.15163\n",
      "[478]\ttest-rmse:0.15163\n",
      "[479]\ttest-rmse:0.15162\n",
      "[480]\ttest-rmse:0.15161\n",
      "[481]\ttest-rmse:0.15161\n",
      "[482]\ttest-rmse:0.15160\n",
      "[483]\ttest-rmse:0.15160\n",
      "[484]\ttest-rmse:0.15160\n",
      "[485]\ttest-rmse:0.15159\n",
      "[486]\ttest-rmse:0.15158\n",
      "[487]\ttest-rmse:0.15157\n",
      "[488]\ttest-rmse:0.15156\n",
      "[489]\ttest-rmse:0.15156\n",
      "[490]\ttest-rmse:0.15156\n",
      "[491]\ttest-rmse:0.15154\n",
      "[492]\ttest-rmse:0.15154\n",
      "[493]\ttest-rmse:0.15153\n",
      "[494]\ttest-rmse:0.15153\n",
      "[495]\ttest-rmse:0.15153\n",
      "[496]\ttest-rmse:0.15152\n",
      "[497]\ttest-rmse:0.15151\n",
      "[498]\ttest-rmse:0.15151\n",
      "[499]\ttest-rmse:0.15151\n",
      "[500]\ttest-rmse:0.15150\n",
      "[501]\ttest-rmse:0.15150\n",
      "[502]\ttest-rmse:0.15150\n",
      "[503]\ttest-rmse:0.15149\n",
      "[504]\ttest-rmse:0.15148\n",
      "[505]\ttest-rmse:0.15147\n",
      "[506]\ttest-rmse:0.15146\n",
      "[507]\ttest-rmse:0.15146\n",
      "[508]\ttest-rmse:0.15146\n",
      "[509]\ttest-rmse:0.15145\n",
      "[510]\ttest-rmse:0.15144\n",
      "[511]\ttest-rmse:0.15144\n",
      "[512]\ttest-rmse:0.15143\n",
      "[513]\ttest-rmse:0.15142\n",
      "[514]\ttest-rmse:0.15141\n",
      "[515]\ttest-rmse:0.15141\n",
      "[516]\ttest-rmse:0.15140\n",
      "[517]\ttest-rmse:0.15140\n",
      "[518]\ttest-rmse:0.15140\n",
      "[519]\ttest-rmse:0.15139\n",
      "[520]\ttest-rmse:0.15139\n",
      "[521]\ttest-rmse:0.15139\n",
      "[522]\ttest-rmse:0.15139\n",
      "[523]\ttest-rmse:0.15138\n",
      "[524]\ttest-rmse:0.15137\n",
      "[525]\ttest-rmse:0.15137\n",
      "[526]\ttest-rmse:0.15136\n",
      "[527]\ttest-rmse:0.15136\n",
      "[528]\ttest-rmse:0.15136\n",
      "[529]\ttest-rmse:0.15135\n",
      "[530]\ttest-rmse:0.15134\n",
      "[531]\ttest-rmse:0.15134\n",
      "[532]\ttest-rmse:0.15133\n",
      "[533]\ttest-rmse:0.15133\n",
      "[534]\ttest-rmse:0.15132\n",
      "[535]\ttest-rmse:0.15132\n",
      "[536]\ttest-rmse:0.15131\n",
      "[537]\ttest-rmse:0.15130\n",
      "[538]\ttest-rmse:0.15130\n",
      "[539]\ttest-rmse:0.15129\n",
      "[540]\ttest-rmse:0.15129\n",
      "[541]\ttest-rmse:0.15129\n",
      "[542]\ttest-rmse:0.15128\n",
      "[543]\ttest-rmse:0.15127\n",
      "[544]\ttest-rmse:0.15126\n",
      "[545]\ttest-rmse:0.15126\n",
      "[546]\ttest-rmse:0.15126\n",
      "[547]\ttest-rmse:0.15125\n",
      "[548]\ttest-rmse:0.15124\n",
      "[549]\ttest-rmse:0.15124\n",
      "[550]\ttest-rmse:0.15124\n",
      "[551]\ttest-rmse:0.15123\n",
      "[552]\ttest-rmse:0.15123\n",
      "[553]\ttest-rmse:0.15122\n",
      "[554]\ttest-rmse:0.15122\n",
      "[555]\ttest-rmse:0.15121\n",
      "[556]\ttest-rmse:0.15121\n",
      "[557]\ttest-rmse:0.15120\n",
      "[558]\ttest-rmse:0.15119\n",
      "[559]\ttest-rmse:0.15118\n",
      "[560]\ttest-rmse:0.15116\n",
      "[561]\ttest-rmse:0.15116\n",
      "[562]\ttest-rmse:0.15115\n",
      "[563]\ttest-rmse:0.15115\n",
      "[564]\ttest-rmse:0.15114\n",
      "[565]\ttest-rmse:0.15113\n",
      "[566]\ttest-rmse:0.15112\n",
      "[567]\ttest-rmse:0.15112\n",
      "[568]\ttest-rmse:0.15112\n",
      "[569]\ttest-rmse:0.15112\n",
      "[570]\ttest-rmse:0.15111\n",
      "[571]\ttest-rmse:0.15112\n",
      "[572]\ttest-rmse:0.15111\n",
      "[573]\ttest-rmse:0.15111\n",
      "[574]\ttest-rmse:0.15111\n",
      "[575]\ttest-rmse:0.15110\n",
      "[576]\ttest-rmse:0.15110\n",
      "[577]\ttest-rmse:0.15109\n",
      "[578]\ttest-rmse:0.15109\n",
      "[579]\ttest-rmse:0.15109\n",
      "[580]\ttest-rmse:0.15108\n",
      "[581]\ttest-rmse:0.15108\n",
      "[582]\ttest-rmse:0.15108\n",
      "[583]\ttest-rmse:0.15107\n",
      "[584]\ttest-rmse:0.15106\n",
      "[585]\ttest-rmse:0.15106\n",
      "[586]\ttest-rmse:0.15106\n",
      "[587]\ttest-rmse:0.15105\n",
      "[588]\ttest-rmse:0.15105\n",
      "[589]\ttest-rmse:0.15105\n",
      "[590]\ttest-rmse:0.15105\n",
      "[591]\ttest-rmse:0.15105\n",
      "[592]\ttest-rmse:0.15104\n",
      "[593]\ttest-rmse:0.15104\n",
      "[594]\ttest-rmse:0.15104\n",
      "[595]\ttest-rmse:0.15103\n",
      "[596]\ttest-rmse:0.15103\n",
      "[597]\ttest-rmse:0.15102\n",
      "[598]\ttest-rmse:0.15102\n",
      "[599]\ttest-rmse:0.15101\n",
      "[600]\ttest-rmse:0.15100\n",
      "[601]\ttest-rmse:0.15100\n",
      "[602]\ttest-rmse:0.15099\n",
      "[603]\ttest-rmse:0.15099\n",
      "[604]\ttest-rmse:0.15098\n",
      "[605]\ttest-rmse:0.15098\n",
      "[606]\ttest-rmse:0.15098\n",
      "[607]\ttest-rmse:0.15097\n",
      "[608]\ttest-rmse:0.15096\n",
      "[609]\ttest-rmse:0.15096\n",
      "[610]\ttest-rmse:0.15096\n",
      "[611]\ttest-rmse:0.15096\n",
      "[612]\ttest-rmse:0.15096\n",
      "[613]\ttest-rmse:0.15095\n",
      "[614]\ttest-rmse:0.15094\n",
      "[615]\ttest-rmse:0.15094\n",
      "[616]\ttest-rmse:0.15094\n",
      "[617]\ttest-rmse:0.15093\n",
      "[618]\ttest-rmse:0.15093\n",
      "[619]\ttest-rmse:0.15092\n",
      "[620]\ttest-rmse:0.15092\n",
      "[621]\ttest-rmse:0.15092\n",
      "[622]\ttest-rmse:0.15092\n",
      "[623]\ttest-rmse:0.15091\n",
      "[624]\ttest-rmse:0.15091\n",
      "[625]\ttest-rmse:0.15091\n",
      "[626]\ttest-rmse:0.15091\n",
      "[627]\ttest-rmse:0.15091\n",
      "[628]\ttest-rmse:0.15091\n",
      "[629]\ttest-rmse:0.15090\n",
      "[630]\ttest-rmse:0.15089\n",
      "[631]\ttest-rmse:0.15089\n",
      "[632]\ttest-rmse:0.15089\n",
      "[633]\ttest-rmse:0.15089\n",
      "[634]\ttest-rmse:0.15088\n",
      "[635]\ttest-rmse:0.15087\n",
      "[636]\ttest-rmse:0.15087\n",
      "[637]\ttest-rmse:0.15087\n",
      "[638]\ttest-rmse:0.15086\n",
      "[639]\ttest-rmse:0.15085\n",
      "[640]\ttest-rmse:0.15084\n",
      "[641]\ttest-rmse:0.15084\n",
      "[642]\ttest-rmse:0.15084\n",
      "[643]\ttest-rmse:0.15083\n",
      "[644]\ttest-rmse:0.15083\n",
      "[645]\ttest-rmse:0.15083\n",
      "[646]\ttest-rmse:0.15083\n",
      "[647]\ttest-rmse:0.15082\n",
      "[648]\ttest-rmse:0.15082\n",
      "[649]\ttest-rmse:0.15082\n",
      "[650]\ttest-rmse:0.15082\n",
      "[651]\ttest-rmse:0.15081\n",
      "[652]\ttest-rmse:0.15080\n",
      "[653]\ttest-rmse:0.15079\n",
      "[654]\ttest-rmse:0.15079\n",
      "[655]\ttest-rmse:0.15078\n",
      "[656]\ttest-rmse:0.15078\n",
      "[657]\ttest-rmse:0.15078\n",
      "[658]\ttest-rmse:0.15078\n",
      "[659]\ttest-rmse:0.15077\n",
      "[660]\ttest-rmse:0.15077\n",
      "[661]\ttest-rmse:0.15077\n",
      "[662]\ttest-rmse:0.15077\n",
      "[663]\ttest-rmse:0.15077\n",
      "[664]\ttest-rmse:0.15076\n",
      "[665]\ttest-rmse:0.15075\n",
      "[666]\ttest-rmse:0.15075\n",
      "[667]\ttest-rmse:0.15075\n",
      "[668]\ttest-rmse:0.15074\n",
      "[669]\ttest-rmse:0.15074\n",
      "[670]\ttest-rmse:0.15073\n",
      "[671]\ttest-rmse:0.15073\n",
      "[672]\ttest-rmse:0.15072\n",
      "[673]\ttest-rmse:0.15072\n",
      "[674]\ttest-rmse:0.15072\n",
      "[675]\ttest-rmse:0.15072\n",
      "[676]\ttest-rmse:0.15071\n",
      "[677]\ttest-rmse:0.15071\n",
      "[678]\ttest-rmse:0.15071\n",
      "[679]\ttest-rmse:0.15070\n",
      "[680]\ttest-rmse:0.15070\n",
      "[681]\ttest-rmse:0.15070\n",
      "[682]\ttest-rmse:0.15070\n",
      "[683]\ttest-rmse:0.15069\n",
      "[684]\ttest-rmse:0.15069\n",
      "[685]\ttest-rmse:0.15067\n",
      "[686]\ttest-rmse:0.15067\n",
      "[687]\ttest-rmse:0.15066\n",
      "[688]\ttest-rmse:0.15065\n",
      "[689]\ttest-rmse:0.15065\n",
      "[690]\ttest-rmse:0.15065\n",
      "[691]\ttest-rmse:0.15065\n",
      "[692]\ttest-rmse:0.15065\n",
      "[693]\ttest-rmse:0.15064\n",
      "[694]\ttest-rmse:0.15064\n",
      "[695]\ttest-rmse:0.15064\n",
      "[696]\ttest-rmse:0.15063\n",
      "[697]\ttest-rmse:0.15063\n",
      "[698]\ttest-rmse:0.15062\n",
      "[699]\ttest-rmse:0.15062\n",
      "[700]\ttest-rmse:0.15061\n",
      "[701]\ttest-rmse:0.15060\n",
      "[702]\ttest-rmse:0.15060\n",
      "[703]\ttest-rmse:0.15059\n",
      "[704]\ttest-rmse:0.15059\n",
      "[705]\ttest-rmse:0.15059\n",
      "[706]\ttest-rmse:0.15058\n",
      "[707]\ttest-rmse:0.15058\n",
      "[708]\ttest-rmse:0.15057\n",
      "[709]\ttest-rmse:0.15058\n",
      "[710]\ttest-rmse:0.15057\n",
      "[711]\ttest-rmse:0.15057\n",
      "[712]\ttest-rmse:0.15057\n",
      "[713]\ttest-rmse:0.15056\n",
      "[714]\ttest-rmse:0.15056\n",
      "[715]\ttest-rmse:0.15056\n",
      "[716]\ttest-rmse:0.15055\n",
      "[717]\ttest-rmse:0.15055\n",
      "[718]\ttest-rmse:0.15055\n",
      "[719]\ttest-rmse:0.15054\n",
      "[720]\ttest-rmse:0.15054\n",
      "[721]\ttest-rmse:0.15054\n",
      "[722]\ttest-rmse:0.15053\n",
      "[723]\ttest-rmse:0.15053\n",
      "[724]\ttest-rmse:0.15052\n",
      "[725]\ttest-rmse:0.15052\n",
      "[726]\ttest-rmse:0.15051\n",
      "[727]\ttest-rmse:0.15050\n",
      "[728]\ttest-rmse:0.15050\n",
      "[729]\ttest-rmse:0.15049\n",
      "[730]\ttest-rmse:0.15049\n",
      "[731]\ttest-rmse:0.15048\n",
      "[732]\ttest-rmse:0.15048\n",
      "[733]\ttest-rmse:0.15048\n",
      "[734]\ttest-rmse:0.15048\n",
      "[735]\ttest-rmse:0.15047\n",
      "[736]\ttest-rmse:0.15048\n",
      "[737]\ttest-rmse:0.15047\n",
      "[738]\ttest-rmse:0.15047\n",
      "[739]\ttest-rmse:0.15046\n",
      "[740]\ttest-rmse:0.15046\n",
      "[741]\ttest-rmse:0.15045\n",
      "[742]\ttest-rmse:0.15045\n",
      "[743]\ttest-rmse:0.15045\n",
      "[744]\ttest-rmse:0.15045\n",
      "[745]\ttest-rmse:0.15044\n",
      "[746]\ttest-rmse:0.15044\n",
      "[747]\ttest-rmse:0.15044\n",
      "[748]\ttest-rmse:0.15044\n",
      "[749]\ttest-rmse:0.15043\n",
      "[750]\ttest-rmse:0.15042\n",
      "[751]\ttest-rmse:0.15042\n",
      "[752]\ttest-rmse:0.15042\n",
      "[753]\ttest-rmse:0.15042\n",
      "[754]\ttest-rmse:0.15042\n",
      "[755]\ttest-rmse:0.15041\n",
      "[756]\ttest-rmse:0.15041\n",
      "[757]\ttest-rmse:0.15041\n",
      "[758]\ttest-rmse:0.15040\n",
      "[759]\ttest-rmse:0.15040\n",
      "[760]\ttest-rmse:0.15040\n",
      "[761]\ttest-rmse:0.15040\n",
      "[762]\ttest-rmse:0.15039\n",
      "[763]\ttest-rmse:0.15038\n",
      "[764]\ttest-rmse:0.15038\n",
      "[765]\ttest-rmse:0.15038\n",
      "[766]\ttest-rmse:0.15038\n",
      "[767]\ttest-rmse:0.15037\n",
      "[768]\ttest-rmse:0.15037\n",
      "[769]\ttest-rmse:0.15037\n",
      "[770]\ttest-rmse:0.15037\n",
      "[771]\ttest-rmse:0.15036\n",
      "[772]\ttest-rmse:0.15036\n",
      "[773]\ttest-rmse:0.15035\n",
      "[774]\ttest-rmse:0.15034\n",
      "[775]\ttest-rmse:0.15034\n",
      "[776]\ttest-rmse:0.15033\n",
      "[777]\ttest-rmse:0.15032\n",
      "[778]\ttest-rmse:0.15032\n",
      "[779]\ttest-rmse:0.15032\n",
      "[780]\ttest-rmse:0.15031\n",
      "[781]\ttest-rmse:0.15030\n",
      "[782]\ttest-rmse:0.15030\n",
      "[783]\ttest-rmse:0.15030\n",
      "[784]\ttest-rmse:0.15030\n",
      "[785]\ttest-rmse:0.15029\n",
      "[786]\ttest-rmse:0.15029\n",
      "[787]\ttest-rmse:0.15029\n",
      "[788]\ttest-rmse:0.15029\n",
      "[789]\ttest-rmse:0.15028\n",
      "[790]\ttest-rmse:0.15028\n",
      "[791]\ttest-rmse:0.15028\n",
      "[792]\ttest-rmse:0.15027\n",
      "[793]\ttest-rmse:0.15027\n",
      "[794]\ttest-rmse:0.15027\n",
      "[795]\ttest-rmse:0.15026\n",
      "[796]\ttest-rmse:0.15026\n",
      "[797]\ttest-rmse:0.15026\n",
      "[798]\ttest-rmse:0.15025\n",
      "[799]\ttest-rmse:0.15025\n",
      "[800]\ttest-rmse:0.15025\n",
      "[801]\ttest-rmse:0.15025\n",
      "[802]\ttest-rmse:0.15025\n",
      "[803]\ttest-rmse:0.15025\n",
      "[804]\ttest-rmse:0.15024\n",
      "[805]\ttest-rmse:0.15024\n",
      "[806]\ttest-rmse:0.15023\n",
      "[807]\ttest-rmse:0.15023\n",
      "[808]\ttest-rmse:0.15022\n",
      "[809]\ttest-rmse:0.15021\n",
      "[810]\ttest-rmse:0.15021\n",
      "[811]\ttest-rmse:0.15020\n",
      "[812]\ttest-rmse:0.15020\n",
      "[813]\ttest-rmse:0.15020\n",
      "[814]\ttest-rmse:0.15019\n",
      "[815]\ttest-rmse:0.15019\n",
      "[816]\ttest-rmse:0.15019\n",
      "[817]\ttest-rmse:0.15018\n",
      "[818]\ttest-rmse:0.15017\n",
      "[819]\ttest-rmse:0.15017\n",
      "[820]\ttest-rmse:0.15017\n",
      "[821]\ttest-rmse:0.15017\n",
      "[822]\ttest-rmse:0.15017\n",
      "[823]\ttest-rmse:0.15016\n",
      "[824]\ttest-rmse:0.15015\n",
      "[825]\ttest-rmse:0.15015\n",
      "[826]\ttest-rmse:0.15015\n",
      "[827]\ttest-rmse:0.15015\n",
      "[828]\ttest-rmse:0.15014\n",
      "[829]\ttest-rmse:0.15014\n",
      "[830]\ttest-rmse:0.15014\n",
      "[831]\ttest-rmse:0.15013\n",
      "[832]\ttest-rmse:0.15013\n",
      "[833]\ttest-rmse:0.15013\n",
      "[834]\ttest-rmse:0.15012\n",
      "[835]\ttest-rmse:0.15011\n",
      "[836]\ttest-rmse:0.15011\n",
      "[837]\ttest-rmse:0.15011\n",
      "[838]\ttest-rmse:0.15010\n",
      "[839]\ttest-rmse:0.15010\n",
      "[840]\ttest-rmse:0.15010\n",
      "[841]\ttest-rmse:0.15010\n",
      "[842]\ttest-rmse:0.15010\n",
      "[843]\ttest-rmse:0.15009\n",
      "[844]\ttest-rmse:0.15009\n",
      "[845]\ttest-rmse:0.15008\n",
      "[846]\ttest-rmse:0.15008\n",
      "[847]\ttest-rmse:0.15007\n",
      "[848]\ttest-rmse:0.15006\n",
      "[849]\ttest-rmse:0.15006\n",
      "[850]\ttest-rmse:0.15006\n",
      "[851]\ttest-rmse:0.15006\n",
      "[852]\ttest-rmse:0.15005\n",
      "[853]\ttest-rmse:0.15004\n",
      "[854]\ttest-rmse:0.15004\n",
      "[855]\ttest-rmse:0.15004\n",
      "[856]\ttest-rmse:0.15004\n",
      "[857]\ttest-rmse:0.15003\n",
      "[858]\ttest-rmse:0.15003\n",
      "[859]\ttest-rmse:0.15003\n",
      "[860]\ttest-rmse:0.15003\n",
      "[861]\ttest-rmse:0.15002\n",
      "[862]\ttest-rmse:0.15002\n",
      "[863]\ttest-rmse:0.15002\n",
      "[864]\ttest-rmse:0.15002\n",
      "[865]\ttest-rmse:0.15001\n",
      "[866]\ttest-rmse:0.15000\n",
      "[867]\ttest-rmse:0.15000\n",
      "[868]\ttest-rmse:0.15000\n",
      "[869]\ttest-rmse:0.15000\n",
      "[870]\ttest-rmse:0.14999\n",
      "[871]\ttest-rmse:0.14999\n",
      "[872]\ttest-rmse:0.14999\n",
      "[873]\ttest-rmse:0.14999\n",
      "[874]\ttest-rmse:0.14998\n",
      "[875]\ttest-rmse:0.14998\n",
      "[876]\ttest-rmse:0.14997\n",
      "[877]\ttest-rmse:0.14997\n",
      "[878]\ttest-rmse:0.14997\n",
      "[879]\ttest-rmse:0.14996\n",
      "[880]\ttest-rmse:0.14996\n",
      "[881]\ttest-rmse:0.14996\n",
      "[882]\ttest-rmse:0.14995\n",
      "[883]\ttest-rmse:0.14995\n",
      "[884]\ttest-rmse:0.14994\n",
      "[885]\ttest-rmse:0.14994\n",
      "[886]\ttest-rmse:0.14994\n",
      "[887]\ttest-rmse:0.14993\n",
      "[888]\ttest-rmse:0.14993\n",
      "[889]\ttest-rmse:0.14993\n",
      "[890]\ttest-rmse:0.14993\n",
      "[891]\ttest-rmse:0.14993\n",
      "[892]\ttest-rmse:0.14993\n",
      "[893]\ttest-rmse:0.14993\n",
      "[894]\ttest-rmse:0.14992\n",
      "[895]\ttest-rmse:0.14992\n",
      "[896]\ttest-rmse:0.14992\n",
      "[897]\ttest-rmse:0.14992\n",
      "[898]\ttest-rmse:0.14992\n",
      "[899]\ttest-rmse:0.14991\n",
      "[900]\ttest-rmse:0.14991\n",
      "[901]\ttest-rmse:0.14991\n",
      "[902]\ttest-rmse:0.14991\n",
      "[903]\ttest-rmse:0.14990\n",
      "[904]\ttest-rmse:0.14991\n",
      "[905]\ttest-rmse:0.14991\n",
      "[906]\ttest-rmse:0.14990\n",
      "[907]\ttest-rmse:0.14990\n",
      "[908]\ttest-rmse:0.14989\n",
      "[909]\ttest-rmse:0.14989\n",
      "[910]\ttest-rmse:0.14989\n",
      "[911]\ttest-rmse:0.14989\n",
      "[912]\ttest-rmse:0.14989\n",
      "[913]\ttest-rmse:0.14988\n",
      "[914]\ttest-rmse:0.14988\n",
      "[915]\ttest-rmse:0.14988\n",
      "[916]\ttest-rmse:0.14987\n",
      "[917]\ttest-rmse:0.14988\n",
      "[918]\ttest-rmse:0.14987\n",
      "[919]\ttest-rmse:0.14987\n",
      "[920]\ttest-rmse:0.14987\n",
      "[921]\ttest-rmse:0.14987\n",
      "[922]\ttest-rmse:0.14987\n",
      "[923]\ttest-rmse:0.14987\n",
      "[924]\ttest-rmse:0.14987\n",
      "[925]\ttest-rmse:0.14986\n",
      "[926]\ttest-rmse:0.14986\n",
      "[927]\ttest-rmse:0.14986\n",
      "[928]\ttest-rmse:0.14986\n",
      "[929]\ttest-rmse:0.14986\n",
      "[930]\ttest-rmse:0.14986\n",
      "[931]\ttest-rmse:0.14985\n",
      "[932]\ttest-rmse:0.14985\n",
      "[933]\ttest-rmse:0.14984\n",
      "[934]\ttest-rmse:0.14984\n",
      "[935]\ttest-rmse:0.14983\n",
      "[936]\ttest-rmse:0.14983\n",
      "[937]\ttest-rmse:0.14983\n",
      "[938]\ttest-rmse:0.14983\n",
      "[939]\ttest-rmse:0.14983\n",
      "[940]\ttest-rmse:0.14982\n",
      "[941]\ttest-rmse:0.14982\n",
      "[942]\ttest-rmse:0.14981\n",
      "[943]\ttest-rmse:0.14981\n",
      "[944]\ttest-rmse:0.14981\n",
      "[945]\ttest-rmse:0.14980\n",
      "[946]\ttest-rmse:0.14979\n",
      "[947]\ttest-rmse:0.14979\n",
      "[948]\ttest-rmse:0.14979\n",
      "[949]\ttest-rmse:0.14978\n",
      "[950]\ttest-rmse:0.14979\n",
      "[951]\ttest-rmse:0.14979\n",
      "[952]\ttest-rmse:0.14978\n",
      "[953]\ttest-rmse:0.14977\n",
      "[954]\ttest-rmse:0.14977\n",
      "[955]\ttest-rmse:0.14976\n",
      "[956]\ttest-rmse:0.14976\n",
      "[957]\ttest-rmse:0.14976\n",
      "[958]\ttest-rmse:0.14976\n",
      "[959]\ttest-rmse:0.14976\n",
      "[960]\ttest-rmse:0.14975\n",
      "[961]\ttest-rmse:0.14975\n",
      "[962]\ttest-rmse:0.14975\n",
      "[963]\ttest-rmse:0.14975\n",
      "[964]\ttest-rmse:0.14975\n",
      "[965]\ttest-rmse:0.14975\n",
      "[966]\ttest-rmse:0.14974\n",
      "[967]\ttest-rmse:0.14974\n",
      "[968]\ttest-rmse:0.14974\n",
      "[969]\ttest-rmse:0.14974\n",
      "[970]\ttest-rmse:0.14974\n",
      "[971]\ttest-rmse:0.14974\n",
      "[972]\ttest-rmse:0.14974\n",
      "[973]\ttest-rmse:0.14974\n",
      "[974]\ttest-rmse:0.14973\n",
      "[975]\ttest-rmse:0.14973\n",
      "[976]\ttest-rmse:0.14972\n",
      "[977]\ttest-rmse:0.14972\n",
      "[978]\ttest-rmse:0.14972\n",
      "[979]\ttest-rmse:0.14971\n",
      "[980]\ttest-rmse:0.14970\n",
      "[981]\ttest-rmse:0.14970\n",
      "[982]\ttest-rmse:0.14970\n",
      "[983]\ttest-rmse:0.14970\n",
      "[984]\ttest-rmse:0.14970\n",
      "[985]\ttest-rmse:0.14970\n",
      "[986]\ttest-rmse:0.14970\n",
      "[987]\ttest-rmse:0.14969\n",
      "[988]\ttest-rmse:0.14969\n",
      "[989]\ttest-rmse:0.14969\n",
      "[990]\ttest-rmse:0.14968\n",
      "[991]\ttest-rmse:0.14968\n",
      "[992]\ttest-rmse:0.14968\n",
      "[993]\ttest-rmse:0.14967\n",
      "[994]\ttest-rmse:0.14967\n",
      "[995]\ttest-rmse:0.14966\n",
      "[996]\ttest-rmse:0.14966\n",
      "[997]\ttest-rmse:0.14966\n",
      "[998]\ttest-rmse:0.14966\n",
      "[999]\ttest-rmse:0.14965\n",
      "[1000]\ttest-rmse:0.14965\n",
      "[1001]\ttest-rmse:0.14965\n",
      "[1002]\ttest-rmse:0.14965\n",
      "[1003]\ttest-rmse:0.14965\n",
      "[1004]\ttest-rmse:0.14964\n",
      "[1005]\ttest-rmse:0.14964\n",
      "[1006]\ttest-rmse:0.14964\n",
      "[1007]\ttest-rmse:0.14963\n",
      "[1008]\ttest-rmse:0.14963\n",
      "[1009]\ttest-rmse:0.14963\n",
      "[1010]\ttest-rmse:0.14962\n",
      "[1011]\ttest-rmse:0.14962\n",
      "[1012]\ttest-rmse:0.14961\n",
      "[1013]\ttest-rmse:0.14961\n",
      "[1014]\ttest-rmse:0.14961\n",
      "[1015]\ttest-rmse:0.14961\n",
      "[1016]\ttest-rmse:0.14960\n",
      "[1017]\ttest-rmse:0.14960\n",
      "[1018]\ttest-rmse:0.14959\n",
      "[1019]\ttest-rmse:0.14959\n",
      "[1020]\ttest-rmse:0.14959\n",
      "[1021]\ttest-rmse:0.14959\n",
      "[1022]\ttest-rmse:0.14959\n",
      "[1023]\ttest-rmse:0.14959\n",
      "[1024]\ttest-rmse:0.14958\n",
      "[1025]\ttest-rmse:0.14958\n",
      "[1026]\ttest-rmse:0.14958\n",
      "[1027]\ttest-rmse:0.14958\n",
      "[1028]\ttest-rmse:0.14957\n",
      "[1029]\ttest-rmse:0.14957\n",
      "[1030]\ttest-rmse:0.14956\n",
      "[1031]\ttest-rmse:0.14956\n",
      "[1032]\ttest-rmse:0.14956\n",
      "[1033]\ttest-rmse:0.14956\n",
      "[1034]\ttest-rmse:0.14955\n",
      "[1035]\ttest-rmse:0.14955\n",
      "[1036]\ttest-rmse:0.14955\n",
      "[1037]\ttest-rmse:0.14955\n",
      "[1038]\ttest-rmse:0.14954\n",
      "[1039]\ttest-rmse:0.14954\n",
      "[1040]\ttest-rmse:0.14953\n",
      "[1041]\ttest-rmse:0.14953\n",
      "[1042]\ttest-rmse:0.14952\n",
      "[1043]\ttest-rmse:0.14952\n",
      "[1044]\ttest-rmse:0.14951\n",
      "[1045]\ttest-rmse:0.14951\n",
      "[1046]\ttest-rmse:0.14951\n",
      "[1047]\ttest-rmse:0.14950\n",
      "[1048]\ttest-rmse:0.14950\n",
      "[1049]\ttest-rmse:0.14950\n",
      "[1050]\ttest-rmse:0.14949\n",
      "[1051]\ttest-rmse:0.14949\n",
      "[1052]\ttest-rmse:0.14949\n",
      "[1053]\ttest-rmse:0.14949\n",
      "[1054]\ttest-rmse:0.14948\n",
      "[1055]\ttest-rmse:0.14948\n",
      "[1056]\ttest-rmse:0.14948\n",
      "[1057]\ttest-rmse:0.14947\n",
      "[1058]\ttest-rmse:0.14947\n",
      "[1059]\ttest-rmse:0.14947\n",
      "[1060]\ttest-rmse:0.14947\n",
      "[1061]\ttest-rmse:0.14946\n",
      "[1062]\ttest-rmse:0.14946\n",
      "[1063]\ttest-rmse:0.14945\n",
      "[1064]\ttest-rmse:0.14945\n",
      "[1065]\ttest-rmse:0.14945\n",
      "[1066]\ttest-rmse:0.14944\n",
      "[1067]\ttest-rmse:0.14943\n",
      "[1068]\ttest-rmse:0.14943\n",
      "[1069]\ttest-rmse:0.14943\n",
      "[1070]\ttest-rmse:0.14943\n",
      "[1071]\ttest-rmse:0.14942\n",
      "[1072]\ttest-rmse:0.14942\n",
      "[1073]\ttest-rmse:0.14942\n",
      "[1074]\ttest-rmse:0.14941\n",
      "[1075]\ttest-rmse:0.14941\n",
      "[1076]\ttest-rmse:0.14941\n",
      "[1077]\ttest-rmse:0.14940\n",
      "[1078]\ttest-rmse:0.14940\n",
      "[1079]\ttest-rmse:0.14940\n",
      "[1080]\ttest-rmse:0.14940\n",
      "[1081]\ttest-rmse:0.14939\n",
      "[1082]\ttest-rmse:0.14939\n",
      "[1083]\ttest-rmse:0.14939\n",
      "[1084]\ttest-rmse:0.14938\n",
      "[1085]\ttest-rmse:0.14938\n",
      "[1086]\ttest-rmse:0.14937\n",
      "[1087]\ttest-rmse:0.14937\n",
      "[1088]\ttest-rmse:0.14937\n",
      "[1089]\ttest-rmse:0.14937\n",
      "[1090]\ttest-rmse:0.14936\n",
      "[1091]\ttest-rmse:0.14936\n",
      "[1092]\ttest-rmse:0.14936\n",
      "[1093]\ttest-rmse:0.14936\n",
      "[1094]\ttest-rmse:0.14936\n",
      "[1095]\ttest-rmse:0.14936\n",
      "[1096]\ttest-rmse:0.14936\n",
      "[1097]\ttest-rmse:0.14936\n",
      "[1098]\ttest-rmse:0.14936\n",
      "[1099]\ttest-rmse:0.14935\n",
      "[1100]\ttest-rmse:0.14935\n",
      "[1101]\ttest-rmse:0.14935\n",
      "[1102]\ttest-rmse:0.14935\n",
      "[1103]\ttest-rmse:0.14935\n",
      "[1104]\ttest-rmse:0.14935\n",
      "[1105]\ttest-rmse:0.14934\n",
      "[1106]\ttest-rmse:0.14934\n",
      "[1107]\ttest-rmse:0.14934\n",
      "[1108]\ttest-rmse:0.14934\n",
      "[1109]\ttest-rmse:0.14934\n",
      "[1110]\ttest-rmse:0.14934\n",
      "[1111]\ttest-rmse:0.14933\n",
      "[1112]\ttest-rmse:0.14933\n",
      "[1113]\ttest-rmse:0.14933\n",
      "[1114]\ttest-rmse:0.14933\n",
      "[1115]\ttest-rmse:0.14933\n",
      "[1116]\ttest-rmse:0.14933\n",
      "[1117]\ttest-rmse:0.14933\n",
      "[1118]\ttest-rmse:0.14932\n",
      "[1119]\ttest-rmse:0.14932\n",
      "[1120]\ttest-rmse:0.14931\n",
      "[1121]\ttest-rmse:0.14931\n",
      "[1122]\ttest-rmse:0.14931\n",
      "[1123]\ttest-rmse:0.14931\n",
      "[1124]\ttest-rmse:0.14931\n",
      "[1125]\ttest-rmse:0.14931\n",
      "[1126]\ttest-rmse:0.14930\n",
      "[1127]\ttest-rmse:0.14930\n",
      "[1128]\ttest-rmse:0.14930\n",
      "[1129]\ttest-rmse:0.14930\n",
      "[1130]\ttest-rmse:0.14929\n",
      "[1131]\ttest-rmse:0.14929\n",
      "[1132]\ttest-rmse:0.14929\n",
      "[1133]\ttest-rmse:0.14928\n",
      "[1134]\ttest-rmse:0.14928\n",
      "[1135]\ttest-rmse:0.14927\n",
      "[1136]\ttest-rmse:0.14927\n",
      "[1137]\ttest-rmse:0.14927\n",
      "[1138]\ttest-rmse:0.14927\n",
      "[1139]\ttest-rmse:0.14926\n",
      "[1140]\ttest-rmse:0.14926\n",
      "[1141]\ttest-rmse:0.14925\n",
      "[1142]\ttest-rmse:0.14925\n",
      "[1143]\ttest-rmse:0.14925\n",
      "[1144]\ttest-rmse:0.14924\n",
      "[1145]\ttest-rmse:0.14925\n",
      "[1146]\ttest-rmse:0.14924\n",
      "[1147]\ttest-rmse:0.14924\n",
      "[1148]\ttest-rmse:0.14923\n",
      "[1149]\ttest-rmse:0.14923\n",
      "[1150]\ttest-rmse:0.14923\n",
      "[1151]\ttest-rmse:0.14923\n",
      "[1152]\ttest-rmse:0.14923\n",
      "[1153]\ttest-rmse:0.14923\n",
      "[1154]\ttest-rmse:0.14923\n",
      "[1155]\ttest-rmse:0.14922\n",
      "[1156]\ttest-rmse:0.14922\n",
      "[1157]\ttest-rmse:0.14922\n",
      "[1158]\ttest-rmse:0.14921\n",
      "[1159]\ttest-rmse:0.14921\n",
      "[1160]\ttest-rmse:0.14921\n",
      "[1161]\ttest-rmse:0.14921\n",
      "[1162]\ttest-rmse:0.14921\n",
      "[1163]\ttest-rmse:0.14921\n",
      "[1164]\ttest-rmse:0.14920\n",
      "[1165]\ttest-rmse:0.14920\n",
      "[1166]\ttest-rmse:0.14920\n",
      "[1167]\ttest-rmse:0.14920\n",
      "[1168]\ttest-rmse:0.14920\n",
      "[1169]\ttest-rmse:0.14919\n",
      "[1170]\ttest-rmse:0.14919\n",
      "[1171]\ttest-rmse:0.14919\n",
      "[1172]\ttest-rmse:0.14919\n",
      "[1173]\ttest-rmse:0.14918\n",
      "[1174]\ttest-rmse:0.14918\n",
      "[1175]\ttest-rmse:0.14918\n",
      "[1176]\ttest-rmse:0.14917\n",
      "[1177]\ttest-rmse:0.14917\n",
      "[1178]\ttest-rmse:0.14917\n",
      "[1179]\ttest-rmse:0.14917\n",
      "[1180]\ttest-rmse:0.14917\n",
      "[1181]\ttest-rmse:0.14917\n",
      "[1182]\ttest-rmse:0.14917\n",
      "[1183]\ttest-rmse:0.14916\n",
      "[1184]\ttest-rmse:0.14916\n",
      "[1185]\ttest-rmse:0.14916\n",
      "[1186]\ttest-rmse:0.14916\n",
      "[1187]\ttest-rmse:0.14915\n",
      "[1188]\ttest-rmse:0.14915\n",
      "[1189]\ttest-rmse:0.14915\n",
      "[1190]\ttest-rmse:0.14915\n",
      "[1191]\ttest-rmse:0.14915\n",
      "[1192]\ttest-rmse:0.14915\n",
      "[1193]\ttest-rmse:0.14915\n",
      "[1194]\ttest-rmse:0.14915\n",
      "[1195]\ttest-rmse:0.14914\n",
      "[1196]\ttest-rmse:0.14914\n",
      "[1197]\ttest-rmse:0.14914\n",
      "[1198]\ttest-rmse:0.14914\n",
      "[1199]\ttest-rmse:0.14914\n",
      "[1200]\ttest-rmse:0.14913\n",
      "[1201]\ttest-rmse:0.14913\n",
      "[1202]\ttest-rmse:0.14913\n",
      "[1203]\ttest-rmse:0.14913\n",
      "[1204]\ttest-rmse:0.14913\n",
      "[1205]\ttest-rmse:0.14912\n",
      "[1206]\ttest-rmse:0.14912\n",
      "[1207]\ttest-rmse:0.14912\n",
      "[1208]\ttest-rmse:0.14912\n",
      "[1209]\ttest-rmse:0.14911\n",
      "[1210]\ttest-rmse:0.14911\n",
      "[1211]\ttest-rmse:0.14911\n",
      "[1212]\ttest-rmse:0.14911\n",
      "[1213]\ttest-rmse:0.14911\n",
      "[1214]\ttest-rmse:0.14911\n",
      "[1215]\ttest-rmse:0.14910\n",
      "[1216]\ttest-rmse:0.14910\n",
      "[1217]\ttest-rmse:0.14910\n",
      "[1218]\ttest-rmse:0.14910\n",
      "[1219]\ttest-rmse:0.14910\n",
      "[1220]\ttest-rmse:0.14909\n",
      "[1221]\ttest-rmse:0.14909\n",
      "[1222]\ttest-rmse:0.14909\n",
      "[1223]\ttest-rmse:0.14908\n",
      "[1224]\ttest-rmse:0.14908\n",
      "[1225]\ttest-rmse:0.14908\n",
      "[1226]\ttest-rmse:0.14907\n",
      "[1227]\ttest-rmse:0.14907\n",
      "[1228]\ttest-rmse:0.14907\n",
      "[1229]\ttest-rmse:0.14907\n",
      "[1230]\ttest-rmse:0.14907\n",
      "[1231]\ttest-rmse:0.14907\n",
      "[1232]\ttest-rmse:0.14906\n",
      "[1233]\ttest-rmse:0.14906\n",
      "[1234]\ttest-rmse:0.14906\n",
      "[1235]\ttest-rmse:0.14906\n",
      "[1236]\ttest-rmse:0.14905\n",
      "[1237]\ttest-rmse:0.14905\n",
      "[1238]\ttest-rmse:0.14904\n",
      "[1239]\ttest-rmse:0.14904\n",
      "[1240]\ttest-rmse:0.14904\n",
      "[1241]\ttest-rmse:0.14904\n",
      "[1242]\ttest-rmse:0.14904\n",
      "[1243]\ttest-rmse:0.14904\n",
      "[1244]\ttest-rmse:0.14904\n",
      "[1245]\ttest-rmse:0.14904\n",
      "[1246]\ttest-rmse:0.14904\n",
      "[1247]\ttest-rmse:0.14903\n",
      "[1248]\ttest-rmse:0.14903\n",
      "[1249]\ttest-rmse:0.14903\n",
      "[1250]\ttest-rmse:0.14903\n",
      "[1251]\ttest-rmse:0.14903\n",
      "[1252]\ttest-rmse:0.14903\n",
      "[1253]\ttest-rmse:0.14903\n",
      "[1254]\ttest-rmse:0.14902\n",
      "[1255]\ttest-rmse:0.14903\n",
      "[1256]\ttest-rmse:0.14903\n",
      "[1257]\ttest-rmse:0.14902\n",
      "[1258]\ttest-rmse:0.14902\n",
      "[1259]\ttest-rmse:0.14902\n",
      "[1260]\ttest-rmse:0.14902\n",
      "[1261]\ttest-rmse:0.14902\n",
      "[1262]\ttest-rmse:0.14902\n",
      "[1263]\ttest-rmse:0.14902\n",
      "[1264]\ttest-rmse:0.14902\n",
      "[1265]\ttest-rmse:0.14901\n",
      "[1266]\ttest-rmse:0.14901\n",
      "[1267]\ttest-rmse:0.14900\n",
      "[1268]\ttest-rmse:0.14900\n",
      "[1269]\ttest-rmse:0.14900\n",
      "[1270]\ttest-rmse:0.14900\n",
      "[1271]\ttest-rmse:0.14900\n",
      "[1272]\ttest-rmse:0.14900\n",
      "[1273]\ttest-rmse:0.14899\n",
      "[1274]\ttest-rmse:0.14899\n",
      "[1275]\ttest-rmse:0.14899\n",
      "[1276]\ttest-rmse:0.14898\n",
      "[1277]\ttest-rmse:0.14898\n",
      "[1278]\ttest-rmse:0.14898\n",
      "[1279]\ttest-rmse:0.14898\n",
      "[1280]\ttest-rmse:0.14898\n",
      "[1281]\ttest-rmse:0.14897\n",
      "[1282]\ttest-rmse:0.14897\n",
      "[1283]\ttest-rmse:0.14897\n",
      "[1284]\ttest-rmse:0.14897\n",
      "[1285]\ttest-rmse:0.14897\n",
      "[1286]\ttest-rmse:0.14897\n",
      "[1287]\ttest-rmse:0.14897\n",
      "[1288]\ttest-rmse:0.14896\n",
      "[1289]\ttest-rmse:0.14896\n",
      "[1290]\ttest-rmse:0.14896\n",
      "[1291]\ttest-rmse:0.14895\n",
      "[1292]\ttest-rmse:0.14895\n",
      "[1293]\ttest-rmse:0.14895\n",
      "[1294]\ttest-rmse:0.14895\n",
      "[1295]\ttest-rmse:0.14895\n",
      "[1296]\ttest-rmse:0.14895\n",
      "[1297]\ttest-rmse:0.14894\n",
      "[1298]\ttest-rmse:0.14894\n",
      "[1299]\ttest-rmse:0.14894\n",
      "[1300]\ttest-rmse:0.14894\n",
      "[1301]\ttest-rmse:0.14893\n",
      "[1302]\ttest-rmse:0.14893\n",
      "[1303]\ttest-rmse:0.14894\n",
      "[1304]\ttest-rmse:0.14893\n",
      "[1305]\ttest-rmse:0.14894\n",
      "[1306]\ttest-rmse:0.14893\n",
      "[1307]\ttest-rmse:0.14894\n",
      "[1308]\ttest-rmse:0.14894\n",
      "[1309]\ttest-rmse:0.14893\n",
      "[1310]\ttest-rmse:0.14893\n",
      "[1311]\ttest-rmse:0.14893\n",
      "[1312]\ttest-rmse:0.14893\n",
      "[1313]\ttest-rmse:0.14892\n",
      "[1314]\ttest-rmse:0.14892\n",
      "[1315]\ttest-rmse:0.14892\n",
      "[1316]\ttest-rmse:0.14892\n",
      "[1317]\ttest-rmse:0.14891\n",
      "[1318]\ttest-rmse:0.14891\n",
      "[1319]\ttest-rmse:0.14891\n",
      "[1320]\ttest-rmse:0.14891\n",
      "[1321]\ttest-rmse:0.14890\n",
      "[1322]\ttest-rmse:0.14890\n",
      "[1323]\ttest-rmse:0.14890\n",
      "[1324]\ttest-rmse:0.14890\n",
      "[1325]\ttest-rmse:0.14890\n",
      "[1326]\ttest-rmse:0.14889\n",
      "[1327]\ttest-rmse:0.14889\n",
      "[1328]\ttest-rmse:0.14889\n",
      "[1329]\ttest-rmse:0.14888\n",
      "[1330]\ttest-rmse:0.14888\n",
      "[1331]\ttest-rmse:0.14888\n",
      "[1332]\ttest-rmse:0.14887\n",
      "[1333]\ttest-rmse:0.14887\n",
      "[1334]\ttest-rmse:0.14887\n",
      "[1335]\ttest-rmse:0.14887\n",
      "[1336]\ttest-rmse:0.14887\n",
      "[1337]\ttest-rmse:0.14886\n",
      "[1338]\ttest-rmse:0.14886\n",
      "[1339]\ttest-rmse:0.14886\n",
      "[1340]\ttest-rmse:0.14886\n",
      "[1341]\ttest-rmse:0.14885\n",
      "[1342]\ttest-rmse:0.14885\n",
      "[1343]\ttest-rmse:0.14885\n",
      "[1344]\ttest-rmse:0.14885\n",
      "[1345]\ttest-rmse:0.14885\n",
      "[1346]\ttest-rmse:0.14884\n",
      "[1347]\ttest-rmse:0.14884\n",
      "[1348]\ttest-rmse:0.14884\n",
      "[1349]\ttest-rmse:0.14883\n",
      "[1350]\ttest-rmse:0.14883\n",
      "[1351]\ttest-rmse:0.14883\n",
      "[1352]\ttest-rmse:0.14883\n",
      "[1353]\ttest-rmse:0.14883\n",
      "[1354]\ttest-rmse:0.14883\n",
      "[1355]\ttest-rmse:0.14883\n",
      "[1356]\ttest-rmse:0.14882\n",
      "[1357]\ttest-rmse:0.14882\n",
      "[1358]\ttest-rmse:0.14882\n",
      "[1359]\ttest-rmse:0.14882\n",
      "[1360]\ttest-rmse:0.14881\n",
      "[1361]\ttest-rmse:0.14881\n",
      "[1362]\ttest-rmse:0.14881\n",
      "[1363]\ttest-rmse:0.14881\n",
      "[1364]\ttest-rmse:0.14881\n",
      "[1365]\ttest-rmse:0.14881\n",
      "[1366]\ttest-rmse:0.14881\n",
      "[1367]\ttest-rmse:0.14880\n",
      "[1368]\ttest-rmse:0.14880\n",
      "[1369]\ttest-rmse:0.14880\n",
      "[1370]\ttest-rmse:0.14880\n",
      "[1371]\ttest-rmse:0.14879\n",
      "[1372]\ttest-rmse:0.14880\n",
      "[1373]\ttest-rmse:0.14880\n",
      "[1374]\ttest-rmse:0.14879\n",
      "[1375]\ttest-rmse:0.14879\n",
      "[1376]\ttest-rmse:0.14879\n",
      "[1377]\ttest-rmse:0.14879\n",
      "[1378]\ttest-rmse:0.14879\n",
      "[1379]\ttest-rmse:0.14879\n",
      "[1380]\ttest-rmse:0.14878\n",
      "[1381]\ttest-rmse:0.14878\n",
      "[1382]\ttest-rmse:0.14878\n",
      "[1383]\ttest-rmse:0.14877\n",
      "[1384]\ttest-rmse:0.14877\n",
      "[1385]\ttest-rmse:0.14877\n",
      "[1386]\ttest-rmse:0.14877\n",
      "[1387]\ttest-rmse:0.14876\n",
      "[1388]\ttest-rmse:0.14877\n",
      "[1389]\ttest-rmse:0.14877\n",
      "[1390]\ttest-rmse:0.14876\n",
      "[1391]\ttest-rmse:0.14876\n",
      "[1392]\ttest-rmse:0.14876\n",
      "[1393]\ttest-rmse:0.14876\n",
      "[1394]\ttest-rmse:0.14876\n",
      "[1395]\ttest-rmse:0.14875\n",
      "[1396]\ttest-rmse:0.14875\n",
      "[1397]\ttest-rmse:0.14875\n",
      "[1398]\ttest-rmse:0.14875\n",
      "[1399]\ttest-rmse:0.14875\n",
      "[1400]\ttest-rmse:0.14874\n",
      "[1401]\ttest-rmse:0.14874\n",
      "[1402]\ttest-rmse:0.14874\n",
      "[1403]\ttest-rmse:0.14873\n",
      "[1404]\ttest-rmse:0.14873\n",
      "[1405]\ttest-rmse:0.14873\n",
      "[1406]\ttest-rmse:0.14873\n",
      "[1407]\ttest-rmse:0.14873\n",
      "[1408]\ttest-rmse:0.14873\n",
      "[1409]\ttest-rmse:0.14873\n",
      "[1410]\ttest-rmse:0.14872\n",
      "[1411]\ttest-rmse:0.14872\n",
      "[1412]\ttest-rmse:0.14872\n",
      "[1413]\ttest-rmse:0.14871\n",
      "[1414]\ttest-rmse:0.14871\n",
      "[1415]\ttest-rmse:0.14871\n",
      "[1416]\ttest-rmse:0.14871\n",
      "[1417]\ttest-rmse:0.14870\n",
      "[1418]\ttest-rmse:0.14870\n",
      "[1419]\ttest-rmse:0.14870\n",
      "[1420]\ttest-rmse:0.14870\n",
      "[1421]\ttest-rmse:0.14869\n",
      "[1422]\ttest-rmse:0.14869\n",
      "[1423]\ttest-rmse:0.14869\n",
      "[1424]\ttest-rmse:0.14868\n",
      "[1425]\ttest-rmse:0.14869\n",
      "[1426]\ttest-rmse:0.14869\n",
      "[1427]\ttest-rmse:0.14869\n",
      "[1428]\ttest-rmse:0.14868\n",
      "[1429]\ttest-rmse:0.14867\n",
      "[1430]\ttest-rmse:0.14867\n",
      "[1431]\ttest-rmse:0.14867\n",
      "[1432]\ttest-rmse:0.14867\n",
      "[1433]\ttest-rmse:0.14867\n",
      "[1434]\ttest-rmse:0.14867\n",
      "[1435]\ttest-rmse:0.14867\n",
      "[1436]\ttest-rmse:0.14867\n",
      "[1437]\ttest-rmse:0.14866\n",
      "[1438]\ttest-rmse:0.14866\n",
      "[1439]\ttest-rmse:0.14866\n",
      "[1440]\ttest-rmse:0.14866\n",
      "[1441]\ttest-rmse:0.14865\n",
      "[1442]\ttest-rmse:0.14865\n",
      "[1443]\ttest-rmse:0.14865\n",
      "[1444]\ttest-rmse:0.14865\n",
      "[1445]\ttest-rmse:0.14865\n",
      "[1446]\ttest-rmse:0.14864\n",
      "[1447]\ttest-rmse:0.14864\n",
      "[1448]\ttest-rmse:0.14864\n",
      "[1449]\ttest-rmse:0.14864\n",
      "[1450]\ttest-rmse:0.14864\n",
      "[1451]\ttest-rmse:0.14864\n",
      "[1452]\ttest-rmse:0.14864\n",
      "[1453]\ttest-rmse:0.14863\n",
      "[1454]\ttest-rmse:0.14863\n",
      "[1455]\ttest-rmse:0.14863\n",
      "[1456]\ttest-rmse:0.14864\n",
      "[1457]\ttest-rmse:0.14864\n",
      "[1458]\ttest-rmse:0.14863\n",
      "[1459]\ttest-rmse:0.14863\n",
      "[1460]\ttest-rmse:0.14863\n",
      "[1461]\ttest-rmse:0.14863\n",
      "[1462]\ttest-rmse:0.14863\n",
      "[1463]\ttest-rmse:0.14862\n",
      "[1464]\ttest-rmse:0.14863\n",
      "[1465]\ttest-rmse:0.14862\n",
      "[1466]\ttest-rmse:0.14862\n",
      "[1467]\ttest-rmse:0.14862\n",
      "[1468]\ttest-rmse:0.14862\n",
      "[1469]\ttest-rmse:0.14862\n",
      "[1470]\ttest-rmse:0.14862\n",
      "[1471]\ttest-rmse:0.14862\n",
      "[1472]\ttest-rmse:0.14862\n",
      "[1473]\ttest-rmse:0.14861\n",
      "[1474]\ttest-rmse:0.14861\n",
      "[1475]\ttest-rmse:0.14861\n",
      "[1476]\ttest-rmse:0.14861\n",
      "[1477]\ttest-rmse:0.14860\n",
      "[1478]\ttest-rmse:0.14860\n",
      "[1479]\ttest-rmse:0.14860\n",
      "[1480]\ttest-rmse:0.14860\n",
      "[1481]\ttest-rmse:0.14860\n",
      "[1482]\ttest-rmse:0.14860\n",
      "[1483]\ttest-rmse:0.14860\n",
      "[1484]\ttest-rmse:0.14860\n",
      "[1485]\ttest-rmse:0.14860\n",
      "[1486]\ttest-rmse:0.14859\n",
      "[1487]\ttest-rmse:0.14859\n",
      "[1488]\ttest-rmse:0.14859\n",
      "[1489]\ttest-rmse:0.14859\n",
      "[1490]\ttest-rmse:0.14859\n",
      "[1491]\ttest-rmse:0.14859\n",
      "[1492]\ttest-rmse:0.14858\n",
      "[1493]\ttest-rmse:0.14858\n",
      "[1494]\ttest-rmse:0.14858\n",
      "[1495]\ttest-rmse:0.14858\n",
      "[1496]\ttest-rmse:0.14858\n",
      "[1497]\ttest-rmse:0.14857\n",
      "[1498]\ttest-rmse:0.14857\n",
      "[1499]\ttest-rmse:0.14857\n",
      "[1500]\ttest-rmse:0.14857\n",
      "[1501]\ttest-rmse:0.14857\n",
      "[1502]\ttest-rmse:0.14856\n",
      "[1503]\ttest-rmse:0.14856\n",
      "[1504]\ttest-rmse:0.14856\n",
      "[1505]\ttest-rmse:0.14856\n",
      "[1506]\ttest-rmse:0.14856\n",
      "[1507]\ttest-rmse:0.14856\n",
      "[1508]\ttest-rmse:0.14856\n",
      "[1509]\ttest-rmse:0.14855\n",
      "[1510]\ttest-rmse:0.14855\n",
      "[1511]\ttest-rmse:0.14855\n",
      "[1512]\ttest-rmse:0.14855\n",
      "[1513]\ttest-rmse:0.14855\n",
      "[1514]\ttest-rmse:0.14855\n",
      "[1515]\ttest-rmse:0.14855\n",
      "[1516]\ttest-rmse:0.14855\n",
      "[1517]\ttest-rmse:0.14854\n",
      "[1518]\ttest-rmse:0.14854\n",
      "[1519]\ttest-rmse:0.14854\n",
      "[1520]\ttest-rmse:0.14853\n",
      "[1521]\ttest-rmse:0.14853\n",
      "[1522]\ttest-rmse:0.14853\n",
      "[1523]\ttest-rmse:0.14853\n",
      "[1524]\ttest-rmse:0.14852\n",
      "[1525]\ttest-rmse:0.14852\n",
      "[1526]\ttest-rmse:0.14852\n",
      "[1527]\ttest-rmse:0.14852\n",
      "[1528]\ttest-rmse:0.14852\n",
      "[1529]\ttest-rmse:0.14851\n",
      "[1530]\ttest-rmse:0.14851\n",
      "[1531]\ttest-rmse:0.14851\n",
      "[1532]\ttest-rmse:0.14851\n",
      "[1533]\ttest-rmse:0.14850\n",
      "[1534]\ttest-rmse:0.14850\n",
      "[1535]\ttest-rmse:0.14850\n",
      "[1536]\ttest-rmse:0.14850\n",
      "[1537]\ttest-rmse:0.14850\n",
      "[1538]\ttest-rmse:0.14850\n",
      "[1539]\ttest-rmse:0.14850\n",
      "[1540]\ttest-rmse:0.14850\n",
      "[1541]\ttest-rmse:0.14850\n",
      "[1542]\ttest-rmse:0.14849\n",
      "[1543]\ttest-rmse:0.14849\n",
      "[1544]\ttest-rmse:0.14849\n",
      "[1545]\ttest-rmse:0.14849\n",
      "[1546]\ttest-rmse:0.14849\n",
      "[1547]\ttest-rmse:0.14849\n",
      "[1548]\ttest-rmse:0.14849\n",
      "[1549]\ttest-rmse:0.14849\n",
      "[1550]\ttest-rmse:0.14848\n",
      "[1551]\ttest-rmse:0.14848\n",
      "[1552]\ttest-rmse:0.14848\n",
      "[1553]\ttest-rmse:0.14847\n",
      "[1554]\ttest-rmse:0.14847\n",
      "[1555]\ttest-rmse:0.14847\n",
      "[1556]\ttest-rmse:0.14847\n",
      "[1557]\ttest-rmse:0.14847\n",
      "[1558]\ttest-rmse:0.14846\n",
      "[1559]\ttest-rmse:0.14846\n",
      "[1560]\ttest-rmse:0.14847\n",
      "[1561]\ttest-rmse:0.14847\n",
      "[1562]\ttest-rmse:0.14846\n",
      "[1563]\ttest-rmse:0.14847\n",
      "[1564]\ttest-rmse:0.14846\n",
      "[1565]\ttest-rmse:0.14846\n",
      "[1566]\ttest-rmse:0.14846\n",
      "[1567]\ttest-rmse:0.14846\n",
      "[1568]\ttest-rmse:0.14846\n",
      "[1569]\ttest-rmse:0.14846\n",
      "[1570]\ttest-rmse:0.14846\n",
      "[1571]\ttest-rmse:0.14846\n",
      "[1572]\ttest-rmse:0.14845\n",
      "[1573]\ttest-rmse:0.14845\n",
      "[1574]\ttest-rmse:0.14845\n",
      "[1575]\ttest-rmse:0.14845\n",
      "[1576]\ttest-rmse:0.14844\n",
      "[1577]\ttest-rmse:0.14845\n",
      "[1578]\ttest-rmse:0.14844\n",
      "[1579]\ttest-rmse:0.14844\n",
      "[1580]\ttest-rmse:0.14843\n",
      "[1581]\ttest-rmse:0.14843\n",
      "[1582]\ttest-rmse:0.14843\n",
      "[1583]\ttest-rmse:0.14843\n",
      "[1584]\ttest-rmse:0.14843\n",
      "[1585]\ttest-rmse:0.14843\n",
      "[1586]\ttest-rmse:0.14843\n",
      "[1587]\ttest-rmse:0.14843\n",
      "[1588]\ttest-rmse:0.14843\n",
      "[1589]\ttest-rmse:0.14842\n",
      "[1590]\ttest-rmse:0.14842\n",
      "[1591]\ttest-rmse:0.14842\n",
      "[1592]\ttest-rmse:0.14842\n",
      "[1593]\ttest-rmse:0.14842\n",
      "[1594]\ttest-rmse:0.14842\n",
      "[1595]\ttest-rmse:0.14842\n",
      "[1596]\ttest-rmse:0.14842\n",
      "[1597]\ttest-rmse:0.14842\n",
      "[1598]\ttest-rmse:0.14842\n",
      "[1599]\ttest-rmse:0.14841\n",
      "[1600]\ttest-rmse:0.14841\n",
      "[1601]\ttest-rmse:0.14841\n",
      "[1602]\ttest-rmse:0.14841\n",
      "[1603]\ttest-rmse:0.14841\n",
      "[1604]\ttest-rmse:0.14840\n",
      "[1605]\ttest-rmse:0.14840\n",
      "[1606]\ttest-rmse:0.14840\n",
      "[1607]\ttest-rmse:0.14840\n",
      "[1608]\ttest-rmse:0.14840\n",
      "[1609]\ttest-rmse:0.14840\n",
      "[1610]\ttest-rmse:0.14840\n",
      "[1611]\ttest-rmse:0.14839\n",
      "[1612]\ttest-rmse:0.14839\n",
      "[1613]\ttest-rmse:0.14839\n",
      "[1614]\ttest-rmse:0.14839\n",
      "[1615]\ttest-rmse:0.14839\n",
      "[1616]\ttest-rmse:0.14838\n",
      "[1617]\ttest-rmse:0.14838\n",
      "[1618]\ttest-rmse:0.14838\n",
      "[1619]\ttest-rmse:0.14838\n",
      "[1620]\ttest-rmse:0.14837\n",
      "[1621]\ttest-rmse:0.14837\n",
      "[1622]\ttest-rmse:0.14837\n",
      "[1623]\ttest-rmse:0.14837\n",
      "[1624]\ttest-rmse:0.14837\n",
      "[1625]\ttest-rmse:0.14836\n",
      "[1626]\ttest-rmse:0.14837\n",
      "[1627]\ttest-rmse:0.14837\n",
      "[1628]\ttest-rmse:0.14837\n",
      "[1629]\ttest-rmse:0.14837\n",
      "[1630]\ttest-rmse:0.14836\n",
      "[1631]\ttest-rmse:0.14836\n",
      "[1632]\ttest-rmse:0.14836\n",
      "[1633]\ttest-rmse:0.14836\n",
      "[1634]\ttest-rmse:0.14836\n",
      "[1635]\ttest-rmse:0.14836\n",
      "[1636]\ttest-rmse:0.14836\n",
      "[1637]\ttest-rmse:0.14835\n",
      "[1638]\ttest-rmse:0.14835\n",
      "[1639]\ttest-rmse:0.14835\n",
      "[1640]\ttest-rmse:0.14835\n",
      "[1641]\ttest-rmse:0.14835\n",
      "[1642]\ttest-rmse:0.14835\n",
      "[1643]\ttest-rmse:0.14834\n",
      "[1644]\ttest-rmse:0.14834\n",
      "[1645]\ttest-rmse:0.14834\n",
      "[1646]\ttest-rmse:0.14834\n",
      "[1647]\ttest-rmse:0.14834\n",
      "[1648]\ttest-rmse:0.14834\n",
      "[1649]\ttest-rmse:0.14834\n",
      "[1650]\ttest-rmse:0.14834\n",
      "[1651]\ttest-rmse:0.14834\n",
      "[1652]\ttest-rmse:0.14834\n",
      "[1653]\ttest-rmse:0.14834\n",
      "[1654]\ttest-rmse:0.14834\n",
      "[1655]\ttest-rmse:0.14834\n",
      "[1656]\ttest-rmse:0.14834\n",
      "[1657]\ttest-rmse:0.14834\n",
      "[1658]\ttest-rmse:0.14833\n",
      "[1659]\ttest-rmse:0.14833\n",
      "[1660]\ttest-rmse:0.14833\n",
      "[1661]\ttest-rmse:0.14833\n",
      "[1662]\ttest-rmse:0.14833\n",
      "[1663]\ttest-rmse:0.14833\n",
      "[1664]\ttest-rmse:0.14833\n",
      "[1665]\ttest-rmse:0.14833\n",
      "[1666]\ttest-rmse:0.14832\n",
      "[1667]\ttest-rmse:0.14832\n",
      "[1668]\ttest-rmse:0.14832\n",
      "[1669]\ttest-rmse:0.14832\n",
      "[1670]\ttest-rmse:0.14832\n",
      "[1671]\ttest-rmse:0.14832\n",
      "[1672]\ttest-rmse:0.14832\n",
      "[1673]\ttest-rmse:0.14832\n",
      "[1674]\ttest-rmse:0.14832\n",
      "[1675]\ttest-rmse:0.14832\n",
      "[1676]\ttest-rmse:0.14832\n",
      "[1677]\ttest-rmse:0.14832\n",
      "[1678]\ttest-rmse:0.14832\n",
      "[1679]\ttest-rmse:0.14832\n",
      "[1680]\ttest-rmse:0.14832\n",
      "[1681]\ttest-rmse:0.14831\n",
      "[1682]\ttest-rmse:0.14831\n",
      "[1683]\ttest-rmse:0.14831\n",
      "[1684]\ttest-rmse:0.14831\n",
      "[1685]\ttest-rmse:0.14831\n",
      "[1686]\ttest-rmse:0.14831\n",
      "[1687]\ttest-rmse:0.14831\n",
      "[1688]\ttest-rmse:0.14830\n",
      "[1689]\ttest-rmse:0.14830\n",
      "[1690]\ttest-rmse:0.14830\n",
      "[1691]\ttest-rmse:0.14830\n",
      "[1692]\ttest-rmse:0.14830\n",
      "[1693]\ttest-rmse:0.14830\n",
      "[1694]\ttest-rmse:0.14830\n",
      "[1695]\ttest-rmse:0.14829\n",
      "[1696]\ttest-rmse:0.14829\n",
      "[1697]\ttest-rmse:0.14829\n",
      "[1698]\ttest-rmse:0.14829\n",
      "[1699]\ttest-rmse:0.14830\n",
      "[1700]\ttest-rmse:0.14829\n",
      "[1701]\ttest-rmse:0.14829\n",
      "[1702]\ttest-rmse:0.14829\n",
      "[1703]\ttest-rmse:0.14829\n",
      "[1704]\ttest-rmse:0.14829\n",
      "[1705]\ttest-rmse:0.14828\n",
      "[1706]\ttest-rmse:0.14828\n",
      "[1707]\ttest-rmse:0.14829\n",
      "[1708]\ttest-rmse:0.14828\n",
      "[1709]\ttest-rmse:0.14828\n",
      "[1710]\ttest-rmse:0.14828\n",
      "[1711]\ttest-rmse:0.14828\n",
      "[1712]\ttest-rmse:0.14828\n",
      "[1713]\ttest-rmse:0.14828\n",
      "[1714]\ttest-rmse:0.14828\n",
      "[1715]\ttest-rmse:0.14828\n",
      "[1716]\ttest-rmse:0.14827\n",
      "[1717]\ttest-rmse:0.14827\n",
      "[1718]\ttest-rmse:0.14827\n",
      "[1719]\ttest-rmse:0.14827\n",
      "[1720]\ttest-rmse:0.14827\n",
      "[1721]\ttest-rmse:0.14827\n",
      "[1722]\ttest-rmse:0.14827\n",
      "[1723]\ttest-rmse:0.14827\n",
      "[1724]\ttest-rmse:0.14827\n",
      "[1725]\ttest-rmse:0.14827\n",
      "[1726]\ttest-rmse:0.14827\n",
      "[1727]\ttest-rmse:0.14826\n",
      "[1728]\ttest-rmse:0.14826\n",
      "[1729]\ttest-rmse:0.14826\n",
      "[1730]\ttest-rmse:0.14826\n",
      "[1731]\ttest-rmse:0.14826\n",
      "[1732]\ttest-rmse:0.14826\n",
      "[1733]\ttest-rmse:0.14825\n",
      "[1734]\ttest-rmse:0.14825\n",
      "[1735]\ttest-rmse:0.14825\n",
      "[1736]\ttest-rmse:0.14825\n",
      "[1737]\ttest-rmse:0.14825\n",
      "[1738]\ttest-rmse:0.14825\n",
      "[1739]\ttest-rmse:0.14825\n",
      "[1740]\ttest-rmse:0.14825\n",
      "[1741]\ttest-rmse:0.14825\n",
      "[1742]\ttest-rmse:0.14825\n",
      "[1743]\ttest-rmse:0.14825\n",
      "[1744]\ttest-rmse:0.14824\n",
      "[1745]\ttest-rmse:0.14824\n",
      "[1746]\ttest-rmse:0.14825\n",
      "[1747]\ttest-rmse:0.14824\n",
      "[1748]\ttest-rmse:0.14824\n",
      "[1749]\ttest-rmse:0.14824\n",
      "[1750]\ttest-rmse:0.14824\n",
      "[1751]\ttest-rmse:0.14824\n",
      "[1752]\ttest-rmse:0.14824\n",
      "[1753]\ttest-rmse:0.14824\n",
      "[1754]\ttest-rmse:0.14824\n",
      "[1755]\ttest-rmse:0.14824\n",
      "[1756]\ttest-rmse:0.14824\n",
      "[1757]\ttest-rmse:0.14823\n",
      "[1758]\ttest-rmse:0.14823\n",
      "[1759]\ttest-rmse:0.14823\n",
      "[1760]\ttest-rmse:0.14823\n",
      "[1761]\ttest-rmse:0.14823\n",
      "[1762]\ttest-rmse:0.14823\n",
      "[1763]\ttest-rmse:0.14823\n",
      "[1764]\ttest-rmse:0.14823\n",
      "[1765]\ttest-rmse:0.14823\n",
      "[1766]\ttest-rmse:0.14823\n",
      "[1767]\ttest-rmse:0.14822\n",
      "[1768]\ttest-rmse:0.14822\n",
      "[1769]\ttest-rmse:0.14822\n",
      "[1770]\ttest-rmse:0.14821\n",
      "[1771]\ttest-rmse:0.14821\n",
      "[1772]\ttest-rmse:0.14821\n",
      "[1773]\ttest-rmse:0.14821\n",
      "[1774]\ttest-rmse:0.14821\n",
      "[1775]\ttest-rmse:0.14821\n",
      "[1776]\ttest-rmse:0.14821\n",
      "[1777]\ttest-rmse:0.14821\n",
      "[1778]\ttest-rmse:0.14821\n",
      "[1779]\ttest-rmse:0.14821\n",
      "[1780]\ttest-rmse:0.14820\n",
      "[1781]\ttest-rmse:0.14820\n",
      "[1782]\ttest-rmse:0.14820\n",
      "[1783]\ttest-rmse:0.14820\n",
      "[1784]\ttest-rmse:0.14820\n",
      "[1785]\ttest-rmse:0.14820\n",
      "[1786]\ttest-rmse:0.14819\n",
      "[1787]\ttest-rmse:0.14820\n",
      "[1788]\ttest-rmse:0.14819\n",
      "[1789]\ttest-rmse:0.14819\n",
      "[1790]\ttest-rmse:0.14819\n",
      "[1791]\ttest-rmse:0.14818\n",
      "[1792]\ttest-rmse:0.14819\n",
      "[1793]\ttest-rmse:0.14818\n",
      "[1794]\ttest-rmse:0.14818\n",
      "[1795]\ttest-rmse:0.14818\n",
      "[1796]\ttest-rmse:0.14818\n",
      "[1797]\ttest-rmse:0.14818\n",
      "[1798]\ttest-rmse:0.14818\n",
      "[1799]\ttest-rmse:0.14818\n",
      "[1800]\ttest-rmse:0.14818\n",
      "[1801]\ttest-rmse:0.14817\n",
      "[1802]\ttest-rmse:0.14817\n",
      "[1803]\ttest-rmse:0.14817\n",
      "[1804]\ttest-rmse:0.14817\n",
      "[1805]\ttest-rmse:0.14817\n",
      "[1806]\ttest-rmse:0.14817\n",
      "[1807]\ttest-rmse:0.14817\n",
      "[1808]\ttest-rmse:0.14817\n",
      "[1809]\ttest-rmse:0.14817\n",
      "[1810]\ttest-rmse:0.14817\n",
      "[1811]\ttest-rmse:0.14817\n",
      "[1812]\ttest-rmse:0.14817\n",
      "[1813]\ttest-rmse:0.14817\n",
      "[1814]\ttest-rmse:0.14816\n",
      "[1815]\ttest-rmse:0.14816\n",
      "[1816]\ttest-rmse:0.14816\n",
      "[1817]\ttest-rmse:0.14815\n",
      "[1818]\ttest-rmse:0.14815\n",
      "[1819]\ttest-rmse:0.14815\n",
      "[1820]\ttest-rmse:0.14815\n",
      "[1821]\ttest-rmse:0.14815\n",
      "[1822]\ttest-rmse:0.14815\n",
      "[1823]\ttest-rmse:0.14815\n",
      "[1824]\ttest-rmse:0.14815\n",
      "[1825]\ttest-rmse:0.14815\n",
      "[1826]\ttest-rmse:0.14815\n",
      "[1827]\ttest-rmse:0.14814\n",
      "[1828]\ttest-rmse:0.14814\n",
      "[1829]\ttest-rmse:0.14814\n",
      "[1830]\ttest-rmse:0.14814\n",
      "[1831]\ttest-rmse:0.14813\n",
      "[1832]\ttest-rmse:0.14813\n",
      "[1833]\ttest-rmse:0.14812\n",
      "[1834]\ttest-rmse:0.14812\n",
      "[1835]\ttest-rmse:0.14812\n",
      "[1836]\ttest-rmse:0.14812\n",
      "[1837]\ttest-rmse:0.14812\n",
      "[1838]\ttest-rmse:0.14812\n",
      "[1839]\ttest-rmse:0.14812\n",
      "[1840]\ttest-rmse:0.14812\n",
      "[1841]\ttest-rmse:0.14812\n",
      "[1842]\ttest-rmse:0.14812\n",
      "[1843]\ttest-rmse:0.14811\n",
      "[1844]\ttest-rmse:0.14812\n",
      "[1845]\ttest-rmse:0.14811\n",
      "[1846]\ttest-rmse:0.14811\n",
      "[1847]\ttest-rmse:0.14811\n",
      "[1848]\ttest-rmse:0.14811\n",
      "[1849]\ttest-rmse:0.14811\n",
      "[1850]\ttest-rmse:0.14811\n",
      "[1851]\ttest-rmse:0.14811\n",
      "[1852]\ttest-rmse:0.14811\n",
      "[1853]\ttest-rmse:0.14811\n",
      "[1854]\ttest-rmse:0.14810\n",
      "[1855]\ttest-rmse:0.14811\n",
      "[1856]\ttest-rmse:0.14810\n",
      "[1857]\ttest-rmse:0.14810\n",
      "[1858]\ttest-rmse:0.14810\n",
      "[1859]\ttest-rmse:0.14810\n",
      "[1860]\ttest-rmse:0.14810\n",
      "[1861]\ttest-rmse:0.14810\n",
      "[1862]\ttest-rmse:0.14810\n",
      "[1863]\ttest-rmse:0.14810\n",
      "[1864]\ttest-rmse:0.14810\n",
      "[1865]\ttest-rmse:0.14810\n",
      "[1866]\ttest-rmse:0.14810\n",
      "[1867]\ttest-rmse:0.14810\n",
      "[1868]\ttest-rmse:0.14810\n",
      "[1869]\ttest-rmse:0.14810\n",
      "[1870]\ttest-rmse:0.14810\n",
      "[1871]\ttest-rmse:0.14810\n",
      "[1872]\ttest-rmse:0.14810\n",
      "[1873]\ttest-rmse:0.14810\n",
      "[1874]\ttest-rmse:0.14809\n",
      "[1875]\ttest-rmse:0.14809\n",
      "[1876]\ttest-rmse:0.14809\n",
      "[1877]\ttest-rmse:0.14809\n",
      "[1878]\ttest-rmse:0.14809\n",
      "[1879]\ttest-rmse:0.14808\n",
      "[1880]\ttest-rmse:0.14808\n",
      "[1881]\ttest-rmse:0.14808\n",
      "[1882]\ttest-rmse:0.14808\n",
      "[1883]\ttest-rmse:0.14808\n",
      "[1884]\ttest-rmse:0.14807\n",
      "[1885]\ttest-rmse:0.14807\n",
      "[1886]\ttest-rmse:0.14807\n",
      "[1887]\ttest-rmse:0.14807\n",
      "[1888]\ttest-rmse:0.14807\n",
      "[1889]\ttest-rmse:0.14807\n",
      "[1890]\ttest-rmse:0.14807\n",
      "[1891]\ttest-rmse:0.14807\n",
      "[1892]\ttest-rmse:0.14807\n",
      "[1893]\ttest-rmse:0.14807\n",
      "[1894]\ttest-rmse:0.14807\n",
      "[1895]\ttest-rmse:0.14807\n",
      "[1896]\ttest-rmse:0.14806\n",
      "[1897]\ttest-rmse:0.14806\n",
      "[1898]\ttest-rmse:0.14806\n",
      "[1899]\ttest-rmse:0.14806\n",
      "[1900]\ttest-rmse:0.14806\n",
      "[1901]\ttest-rmse:0.14806\n",
      "[1902]\ttest-rmse:0.14806\n",
      "[1903]\ttest-rmse:0.14806\n",
      "[1904]\ttest-rmse:0.14806\n",
      "[1905]\ttest-rmse:0.14806\n",
      "[1906]\ttest-rmse:0.14806\n",
      "[1907]\ttest-rmse:0.14805\n",
      "[1908]\ttest-rmse:0.14805\n",
      "[1909]\ttest-rmse:0.14806\n",
      "[1910]\ttest-rmse:0.14805\n",
      "[1911]\ttest-rmse:0.14805\n",
      "[1912]\ttest-rmse:0.14805\n",
      "[1913]\ttest-rmse:0.14805\n",
      "[1914]\ttest-rmse:0.14805\n",
      "[1915]\ttest-rmse:0.14805\n",
      "[1916]\ttest-rmse:0.14805\n",
      "[1917]\ttest-rmse:0.14805\n",
      "[1918]\ttest-rmse:0.14805\n",
      "[1919]\ttest-rmse:0.14804\n",
      "[1920]\ttest-rmse:0.14804\n",
      "[1921]\ttest-rmse:0.14804\n",
      "[1922]\ttest-rmse:0.14804\n",
      "[1923]\ttest-rmse:0.14804\n",
      "[1924]\ttest-rmse:0.14804\n",
      "[1925]\ttest-rmse:0.14804\n",
      "[1926]\ttest-rmse:0.14804\n",
      "[1927]\ttest-rmse:0.14804\n",
      "[1928]\ttest-rmse:0.14803\n",
      "[1929]\ttest-rmse:0.14803\n",
      "[1930]\ttest-rmse:0.14803\n",
      "[1931]\ttest-rmse:0.14803\n",
      "[1932]\ttest-rmse:0.14802\n",
      "[1933]\ttest-rmse:0.14802\n",
      "[1934]\ttest-rmse:0.14802\n",
      "[1935]\ttest-rmse:0.14802\n",
      "[1936]\ttest-rmse:0.14802\n",
      "[1937]\ttest-rmse:0.14802\n",
      "[1938]\ttest-rmse:0.14802\n",
      "[1939]\ttest-rmse:0.14802\n",
      "[1940]\ttest-rmse:0.14802\n",
      "[1941]\ttest-rmse:0.14802\n",
      "[1942]\ttest-rmse:0.14801\n",
      "[1943]\ttest-rmse:0.14801\n",
      "[1944]\ttest-rmse:0.14801\n",
      "[1945]\ttest-rmse:0.14801\n",
      "[1946]\ttest-rmse:0.14801\n",
      "[1947]\ttest-rmse:0.14801\n",
      "[1948]\ttest-rmse:0.14801\n",
      "[1949]\ttest-rmse:0.14801\n",
      "[1950]\ttest-rmse:0.14801\n",
      "[1951]\ttest-rmse:0.14800\n",
      "[1952]\ttest-rmse:0.14800\n",
      "[1953]\ttest-rmse:0.14800\n",
      "[1954]\ttest-rmse:0.14801\n",
      "[1955]\ttest-rmse:0.14800\n",
      "[1956]\ttest-rmse:0.14801\n",
      "[1957]\ttest-rmse:0.14800\n",
      "[1958]\ttest-rmse:0.14800\n",
      "[1959]\ttest-rmse:0.14800\n",
      "[1960]\ttest-rmse:0.14800\n",
      "[1961]\ttest-rmse:0.14800\n",
      "[1962]\ttest-rmse:0.14800\n",
      "[1963]\ttest-rmse:0.14800\n",
      "[1964]\ttest-rmse:0.14800\n",
      "[1965]\ttest-rmse:0.14800\n",
      "[1966]\ttest-rmse:0.14800\n",
      "[1967]\ttest-rmse:0.14800\n",
      "[1968]\ttest-rmse:0.14799\n",
      "[1969]\ttest-rmse:0.14799\n",
      "[1970]\ttest-rmse:0.14799\n",
      "[1971]\ttest-rmse:0.14799\n",
      "[1972]\ttest-rmse:0.14800\n",
      "[1973]\ttest-rmse:0.14799\n",
      "[1974]\ttest-rmse:0.14799\n",
      "[1975]\ttest-rmse:0.14799\n",
      "[1976]\ttest-rmse:0.14799\n",
      "[1977]\ttest-rmse:0.14799\n",
      "[1978]\ttest-rmse:0.14798\n",
      "[1979]\ttest-rmse:0.14798\n",
      "[1980]\ttest-rmse:0.14798\n",
      "[1981]\ttest-rmse:0.14798\n",
      "[1982]\ttest-rmse:0.14798\n",
      "[1983]\ttest-rmse:0.14798\n",
      "[1984]\ttest-rmse:0.14797\n",
      "[1985]\ttest-rmse:0.14798\n",
      "[1986]\ttest-rmse:0.14797\n",
      "[1987]\ttest-rmse:0.14797\n",
      "[1988]\ttest-rmse:0.14797\n",
      "[1989]\ttest-rmse:0.14797\n",
      "[1990]\ttest-rmse:0.14796\n",
      "[1991]\ttest-rmse:0.14796\n",
      "[1992]\ttest-rmse:0.14797\n",
      "[1993]\ttest-rmse:0.14796\n",
      "[1994]\ttest-rmse:0.14796\n",
      "[1995]\ttest-rmse:0.14796\n",
      "[1996]\ttest-rmse:0.14796\n",
      "[1997]\ttest-rmse:0.14796\n",
      "[1998]\ttest-rmse:0.14796\n",
      "[1999]\ttest-rmse:0.14796\n",
      "Val loss=0.02189, Val MAE=0.11243\n"
     ]
    }
   ],
   "source": [
    "context_length = 32\n",
    "features = ['ValoresNormalizados']\n",
    "    \n",
    "def custom_loss_xgb(y_pred, dtrain, alpha=0.6, beta=0.2, gamma=0.2):\n",
    "    y_true = dtrain.get_label()\n",
    "    error = y_pred - y_true\n",
    "\n",
    "    # --- Huber loss ---\n",
    "    delta = 1.5\n",
    "    abs_error = np.abs(error)\n",
    "    is_small_error = abs_error <= delta\n",
    "\n",
    "    # Gradiente Huber\n",
    "    grad_huber = np.where(is_small_error, error, delta * np.sign(error))\n",
    "    # Hessiano Huber\n",
    "    hess_huber = np.where(is_small_error, 1.0, 0.0)\n",
    "\n",
    "    # --- Componente alta precipitación (>0.6) ---\n",
    "    high_mask = (y_true > 0.6).astype(float)\n",
    "    grad_high = 2 * high_mask * error\n",
    "    hess_high = 2 * high_mask\n",
    "\n",
    "    # --- Componente baja precipitación (<0.1) ---\n",
    "    low_mask = (y_true < 0.1).astype(float)\n",
    "    grad_low = 2 * low_mask * error\n",
    "    hess_low = 2 * low_mask\n",
    "\n",
    "    # --- Combinación ponderada ---\n",
    "    grad_total = alpha * grad_huber + beta * grad_high + gamma * grad_low\n",
    "    hess_total = alpha * hess_huber + beta * hess_high + gamma * hess_low\n",
    "\n",
    "    return grad_total, hess_total\n",
    "\n",
    "for feature_column in features:\n",
    "    # Preparar los datos\n",
    "    X_train, X_test, y_train, y_test = split_data(df, context_length=context_length, feature=feature_column)\n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "    print(f\"Forma de X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"Forma de X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "    sample_weights = compute_sample_weights(y_train.flatten())\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train, \n",
    "                         weight=sample_weights\n",
    "                         )\n",
    "    dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "    params = {\n",
    "        'max_depth': 8,\n",
    "        'eta': 0.03,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'eval_metric': 'rmse'\n",
    "    }\n",
    "\n",
    "    model = xgb.train(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=2000,\n",
    "        obj=custom_loss_xgb,\n",
    "        evals=[(dtest, \"test\")],\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=True\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    pred = model.predict(dtest)\n",
    "    loss = mean_squared_error(y_test, pred)\n",
    "    mae = mean_absolute_error(y_test, pred)\n",
    "    print(f\"Val loss={loss:.5f}, Val MAE={mae:.5f}\")\n",
    "\n",
    "    # Guardar el modelo\n",
    "    os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "    model.save_model(os.path.join(MODELS_FOLDER, f\"xgb_{feature_column}.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modo: ValoresNormalizados\n",
      "Forma de X_train: (209570, 32, 1), y_train: (209570, 1)\n",
      "Forma de X_test: (54055, 32, 1), y_test: (54055, 1)\n",
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1750451819.439438   56134 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 22ms/step - loss: 0.0744 - mae: 0.1639 - val_loss: 0.0252 - val_mae: 0.1580 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0150 - mae: 0.1248 - val_loss: 0.0208 - val_mae: 0.1836 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0102 - mae: 0.1210 - val_loss: 0.0114 - val_mae: 0.1201 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0093 - mae: 0.1199 - val_loss: 0.0110 - val_mae: 0.1207 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0090 - mae: 0.1196 - val_loss: 0.0107 - val_mae: 0.1216 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0089 - mae: 0.1193 - val_loss: 0.0107 - val_mae: 0.1250 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0088 - mae: 0.1191 - val_loss: 0.0107 - val_mae: 0.1211 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0088 - mae: 0.1190 - val_loss: 0.0104 - val_mae: 0.1220 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0086 - mae: 0.1184 - val_loss: 0.0106 - val_mae: 0.1206 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0086 - mae: 0.1185 - val_loss: 0.0103 - val_mae: 0.1208 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0086 - mae: 0.1186 - val_loss: 0.0105 - val_mae: 0.1259 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0086 - mae: 0.1186 - val_loss: 0.0104 - val_mae: 0.1195 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0086 - mae: 0.1182 - val_loss: 0.0104 - val_mae: 0.1215 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0085 - mae: 0.1180 - val_loss: 0.0103 - val_mae: 0.1208 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0084 - mae: 0.1179 - val_loss: 0.0102 - val_mae: 0.1196 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0084 - mae: 0.1178 - val_loss: 0.0101 - val_mae: 0.1200 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0084 - mae: 0.1177 - val_loss: 0.0104 - val_mae: 0.1261 - learning_rate: 0.0010\n",
      "Epoch 18/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0083 - mae: 0.1173 - val_loss: 0.0101 - val_mae: 0.1216 - learning_rate: 0.0010\n",
      "Epoch 19/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0083 - mae: 0.1170 - val_loss: 0.0101 - val_mae: 0.1221 - learning_rate: 0.0010\n",
      "Epoch 20/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0082 - mae: 0.1168 - val_loss: 0.0104 - val_mae: 0.1262 - learning_rate: 0.0010\n",
      "Epoch 21/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0083 - mae: 0.1170 - val_loss: 0.0102 - val_mae: 0.1229 - learning_rate: 0.0010\n",
      "Epoch 22/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0082 - mae: 0.1168 - val_loss: 0.0102 - val_mae: 0.1210 - learning_rate: 0.0010\n",
      "Epoch 23/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0082 - mae: 0.1164 - val_loss: 0.0102 - val_mae: 0.1223 - learning_rate: 0.0010\n",
      "Epoch 24/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0082 - mae: 0.1163 - val_loss: 0.0103 - val_mae: 0.1206 - learning_rate: 0.0010\n",
      "Epoch 25/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0081 - mae: 0.1160 - val_loss: 0.0100 - val_mae: 0.1199 - learning_rate: 0.0010\n",
      "Epoch 26/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0080 - mae: 0.1158 - val_loss: 0.0100 - val_mae: 0.1203 - learning_rate: 0.0010\n",
      "Epoch 27/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0080 - mae: 0.1155 - val_loss: 0.0108 - val_mae: 0.1311 - learning_rate: 0.0010\n",
      "Epoch 28/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0081 - mae: 0.1161 - val_loss: 0.0103 - val_mae: 0.1247 - learning_rate: 0.0010\n",
      "Epoch 29/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0080 - mae: 0.1159 - val_loss: 0.0101 - val_mae: 0.1228 - learning_rate: 0.0010\n",
      "Epoch 30/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0080 - mae: 0.1155 - val_loss: 0.0100 - val_mae: 0.1212 - learning_rate: 0.0010\n",
      "Epoch 31/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0080 - mae: 0.1150 - val_loss: 0.0101 - val_mae: 0.1233 - learning_rate: 0.0010\n",
      "Epoch 32/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0080 - mae: 0.1153 - val_loss: 0.0099 - val_mae: 0.1198 - learning_rate: 0.0010\n",
      "Epoch 33/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0079 - mae: 0.1148 - val_loss: 0.0100 - val_mae: 0.1189 - learning_rate: 0.0010\n",
      "Epoch 34/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0080 - mae: 0.1154 - val_loss: 0.0099 - val_mae: 0.1216 - learning_rate: 0.0010\n",
      "Epoch 35/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0079 - mae: 0.1147 - val_loss: 0.0100 - val_mae: 0.1201 - learning_rate: 0.0010\n",
      "Epoch 36/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0078 - mae: 0.1142 - val_loss: 0.0100 - val_mae: 0.1226 - learning_rate: 0.0010\n",
      "Epoch 37/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0079 - mae: 0.1145 - val_loss: 0.0098 - val_mae: 0.1196 - learning_rate: 0.0010\n",
      "Epoch 38/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0079 - mae: 0.1149 - val_loss: 0.0099 - val_mae: 0.1209 - learning_rate: 0.0010\n",
      "Epoch 39/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0078 - mae: 0.1143 - val_loss: 0.0100 - val_mae: 0.1206 - learning_rate: 0.0010\n",
      "Epoch 40/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0079 - mae: 0.1147 - val_loss: 0.0101 - val_mae: 0.1217 - learning_rate: 0.0010\n",
      "Epoch 41/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0078 - mae: 0.1137 - val_loss: 0.0100 - val_mae: 0.1222 - learning_rate: 0.0010\n",
      "Epoch 42/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0079 - mae: 0.1148 - val_loss: 0.0101 - val_mae: 0.1244 - learning_rate: 0.0010\n",
      "Epoch 43/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0077 - mae: 0.1134 - val_loss: 0.0100 - val_mae: 0.1173 - learning_rate: 0.0010\n",
      "Epoch 44/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0078 - mae: 0.1139 - val_loss: 0.0099 - val_mae: 0.1173 - learning_rate: 0.0010\n",
      "Epoch 45/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0077 - mae: 0.1132 - val_loss: 0.0098 - val_mae: 0.1187 - learning_rate: 0.0010\n",
      "Epoch 46/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0077 - mae: 0.1137 - val_loss: 0.0099 - val_mae: 0.1183 - learning_rate: 0.0010\n",
      "Epoch 47/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0078 - mae: 0.1139 - val_loss: 0.0098 - val_mae: 0.1201 - learning_rate: 0.0010\n",
      "Epoch 48/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0077 - mae: 0.1136 - val_loss: 0.0099 - val_mae: 0.1180 - learning_rate: 0.0010\n",
      "Epoch 49/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0077 - mae: 0.1135 - val_loss: 0.0098 - val_mae: 0.1186 - learning_rate: 0.0010\n",
      "Epoch 50/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0076 - mae: 0.1128 - val_loss: 0.0098 - val_mae: 0.1197 - learning_rate: 0.0010\n",
      "Epoch 51/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0076 - mae: 0.1128 - val_loss: 0.0099 - val_mae: 0.1225 - learning_rate: 0.0010\n",
      "Epoch 52/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0076 - mae: 0.1132 - val_loss: 0.0098 - val_mae: 0.1196 - learning_rate: 0.0010\n",
      "Epoch 53/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0076 - mae: 0.1125 - val_loss: 0.0098 - val_mae: 0.1167 - learning_rate: 0.0010\n",
      "Epoch 54/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0076 - mae: 0.1124 - val_loss: 0.0100 - val_mae: 0.1214 - learning_rate: 0.0010\n",
      "Epoch 55/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0076 - mae: 0.1127 - val_loss: 0.0098 - val_mae: 0.1174 - learning_rate: 0.0010\n",
      "Epoch 56/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0075 - mae: 0.1122 - val_loss: 0.0099 - val_mae: 0.1186 - learning_rate: 0.0010\n",
      "Epoch 57/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0076 - mae: 0.1123 - val_loss: 0.0098 - val_mae: 0.1208 - learning_rate: 0.0010\n",
      "Epoch 58/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0075 - mae: 0.1126 - val_loss: 0.0098 - val_mae: 0.1204 - learning_rate: 0.0010\n",
      "Epoch 59/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0076 - mae: 0.1124 - val_loss: 0.0098 - val_mae: 0.1200 - learning_rate: 0.0010\n",
      "Epoch 60/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0074 - mae: 0.1115 - val_loss: 0.0099 - val_mae: 0.1214 - learning_rate: 0.0010\n",
      "Epoch 61/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0075 - mae: 0.1121 - val_loss: 0.0100 - val_mae: 0.1175 - learning_rate: 0.0010\n",
      "Epoch 62/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0075 - mae: 0.1118 - val_loss: 0.0099 - val_mae: 0.1177 - learning_rate: 0.0010\n",
      "Epoch 63/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0075 - mae: 0.1119 - val_loss: 0.0098 - val_mae: 0.1203 - learning_rate: 0.0010\n",
      "Epoch 64/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0075 - mae: 0.1117 - val_loss: 0.0097 - val_mae: 0.1199 - learning_rate: 0.0010\n",
      "Epoch 65/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0075 - mae: 0.1113 - val_loss: 0.0098 - val_mae: 0.1183 - learning_rate: 0.0010\n",
      "Epoch 66/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0074 - mae: 0.1116 - val_loss: 0.0097 - val_mae: 0.1199 - learning_rate: 0.0010\n",
      "Epoch 67/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0074 - mae: 0.1110 - val_loss: 0.0097 - val_mae: 0.1194 - learning_rate: 0.0010\n",
      "Epoch 68/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0074 - mae: 0.1114 - val_loss: 0.0099 - val_mae: 0.1226 - learning_rate: 0.0010\n",
      "Epoch 69/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0073 - mae: 0.1110 - val_loss: 0.0100 - val_mae: 0.1162 - learning_rate: 0.0010\n",
      "Epoch 70/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0073 - mae: 0.1106 - val_loss: 0.0097 - val_mae: 0.1190 - learning_rate: 0.0010\n",
      "Epoch 71/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0073 - mae: 0.1108 - val_loss: 0.0099 - val_mae: 0.1189 - learning_rate: 0.0010\n",
      "Epoch 72/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0073 - mae: 0.1105 - val_loss: 0.0098 - val_mae: 0.1170 - learning_rate: 0.0010\n",
      "Epoch 73/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0073 - mae: 0.1108 - val_loss: 0.0101 - val_mae: 0.1227 - learning_rate: 0.0010\n",
      "Epoch 74/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0074 - mae: 0.1111 - val_loss: 0.0099 - val_mae: 0.1230 - learning_rate: 0.0010\n",
      "Epoch 75/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0073 - mae: 0.1107 - val_loss: 0.0099 - val_mae: 0.1192 - learning_rate: 0.0010\n",
      "Epoch 76/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0073 - mae: 0.1106 - val_loss: 0.0097 - val_mae: 0.1183 - learning_rate: 0.0010\n",
      "Epoch 77/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0072 - mae: 0.1103 - val_loss: 0.0097 - val_mae: 0.1203 - learning_rate: 0.0010\n",
      "Epoch 78/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0072 - mae: 0.1101 - val_loss: 0.0099 - val_mae: 0.1220 - learning_rate: 0.0010\n",
      "Epoch 79/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0073 - mae: 0.1106 - val_loss: 0.0096 - val_mae: 0.1181 - learning_rate: 0.0010\n",
      "Epoch 80/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0072 - mae: 0.1102 - val_loss: 0.0099 - val_mae: 0.1215 - learning_rate: 0.0010\n",
      "Epoch 81/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0072 - mae: 0.1098 - val_loss: 0.0099 - val_mae: 0.1174 - learning_rate: 0.0010\n",
      "Epoch 82/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0072 - mae: 0.1100 - val_loss: 0.0098 - val_mae: 0.1184 - learning_rate: 0.0010\n",
      "Epoch 83/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0072 - mae: 0.1097 - val_loss: 0.0096 - val_mae: 0.1187 - learning_rate: 0.0010\n",
      "Epoch 84/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0071 - mae: 0.1095 - val_loss: 0.0096 - val_mae: 0.1171 - learning_rate: 0.0010\n",
      "Epoch 85/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0072 - mae: 0.1097 - val_loss: 0.0097 - val_mae: 0.1189 - learning_rate: 0.0010\n",
      "Epoch 86/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0071 - mae: 0.1098 - val_loss: 0.0097 - val_mae: 0.1194 - learning_rate: 0.0010\n",
      "Epoch 87/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0071 - mae: 0.1094 - val_loss: 0.0098 - val_mae: 0.1176 - learning_rate: 0.0010\n",
      "Epoch 88/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0072 - mae: 0.1096 - val_loss: 0.0098 - val_mae: 0.1161 - learning_rate: 0.0010\n",
      "Epoch 89/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0071 - mae: 0.1092 - val_loss: 0.0101 - val_mae: 0.1245 - learning_rate: 0.0010\n",
      "Epoch 90/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0071 - mae: 0.1093 - val_loss: 0.0096 - val_mae: 0.1177 - learning_rate: 0.0010\n",
      "Epoch 91/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0070 - mae: 0.1087 - val_loss: 0.0099 - val_mae: 0.1219 - learning_rate: 0.0010\n",
      "Epoch 92/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0072 - mae: 0.1099 - val_loss: 0.0098 - val_mae: 0.1189 - learning_rate: 0.0010\n",
      "Epoch 93/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0071 - mae: 0.1093 - val_loss: 0.0098 - val_mae: 0.1196 - learning_rate: 0.0010\n",
      "Epoch 94/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0071 - mae: 0.1093 - val_loss: 0.0098 - val_mae: 0.1183 - learning_rate: 0.0010\n",
      "Epoch 95/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0071 - mae: 0.1091 - val_loss: 0.0097 - val_mae: 0.1174 - learning_rate: 0.0010\n",
      "Epoch 96/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0070 - mae: 0.1090 - val_loss: 0.0099 - val_mae: 0.1223 - learning_rate: 0.0010\n",
      "Epoch 97/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0070 - mae: 0.1085 - val_loss: 0.0100 - val_mae: 0.1217 - learning_rate: 0.0010\n",
      "Epoch 98/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0070 - mae: 0.1086 - val_loss: 0.0097 - val_mae: 0.1187 - learning_rate: 0.0010\n",
      "Epoch 99/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0070 - mae: 0.1082 - val_loss: 0.0097 - val_mae: 0.1194 - learning_rate: 0.0010\n",
      "Epoch 100/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0070 - mae: 0.1085 - val_loss: 0.0097 - val_mae: 0.1186 - learning_rate: 0.0010\n",
      "Epoch 101/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0070 - mae: 0.1085 - val_loss: 0.0097 - val_mae: 0.1180 - learning_rate: 0.0010\n",
      "Epoch 102/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0071 - mae: 0.1087 - val_loss: 0.0097 - val_mae: 0.1184 - learning_rate: 0.0010\n",
      "Epoch 103/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0069 - mae: 0.1081 - val_loss: 0.0101 - val_mae: 0.1255 - learning_rate: 0.0010\n",
      "Epoch 104/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0070 - mae: 0.1085 - val_loss: 0.0098 - val_mae: 0.1200 - learning_rate: 0.0010\n",
      "Epoch 105/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0070 - mae: 0.1081 - val_loss: 0.0098 - val_mae: 0.1203 - learning_rate: 0.0010\n",
      "Epoch 106/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0069 - mae: 0.1079 - val_loss: 0.0098 - val_mae: 0.1191 - learning_rate: 0.0010\n",
      "Epoch 107/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0069 - mae: 0.1080 - val_loss: 0.0098 - val_mae: 0.1171 - learning_rate: 0.0010\n",
      "Epoch 108/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0069 - mae: 0.1079 - val_loss: 0.0096 - val_mae: 0.1182 - learning_rate: 0.0010\n",
      "Epoch 109/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0069 - mae: 0.1079 - val_loss: 0.0097 - val_mae: 0.1201 - learning_rate: 0.0010\n",
      "Epoch 110/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0069 - mae: 0.1081 - val_loss: 0.0098 - val_mae: 0.1186 - learning_rate: 0.0010\n",
      "Epoch 111/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0068 - mae: 0.1071 - val_loss: 0.0096 - val_mae: 0.1176 - learning_rate: 0.0010\n",
      "Epoch 112/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0069 - mae: 0.1074 - val_loss: 0.0098 - val_mae: 0.1205 - learning_rate: 0.0010\n",
      "Epoch 113/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0068 - mae: 0.1074 - val_loss: 0.0097 - val_mae: 0.1193 - learning_rate: 0.0010\n",
      "Epoch 114/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0066 - mae: 0.1059 - val_loss: 0.0096 - val_mae: 0.1184 - learning_rate: 1.0000e-04\n",
      "Epoch 115/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0065 - mae: 0.1048 - val_loss: 0.0096 - val_mae: 0.1180 - learning_rate: 1.0000e-04\n",
      "Epoch 116/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0065 - mae: 0.1047 - val_loss: 0.0096 - val_mae: 0.1183 - learning_rate: 1.0000e-04\n",
      "Epoch 117/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0064 - mae: 0.1044 - val_loss: 0.0096 - val_mae: 0.1189 - learning_rate: 1.0000e-04\n",
      "Epoch 118/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0064 - mae: 0.1044 - val_loss: 0.0097 - val_mae: 0.1194 - learning_rate: 1.0000e-04\n",
      "Epoch 119/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0064 - mae: 0.1043 - val_loss: 0.0096 - val_mae: 0.1184 - learning_rate: 1.0000e-04\n",
      "Epoch 120/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0064 - mae: 0.1043 - val_loss: 0.0096 - val_mae: 0.1176 - learning_rate: 1.0000e-04\n",
      "Epoch 121/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0064 - mae: 0.1043 - val_loss: 0.0097 - val_mae: 0.1196 - learning_rate: 1.0000e-04\n",
      "Epoch 122/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - loss: 0.0064 - mae: 0.1045 - val_loss: 0.0097 - val_mae: 0.1184 - learning_rate: 1.0000e-04\n",
      "Epoch 123/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 20ms/step - loss: 0.0064 - mae: 0.1044 - val_loss: 0.0096 - val_mae: 0.1183 - learning_rate: 1.0000e-04\n",
      "Epoch 124/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0064 - mae: 0.1042 - val_loss: 0.0096 - val_mae: 0.1185 - learning_rate: 1.0000e-04\n",
      "Epoch 125/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0064 - mae: 0.1043 - val_loss: 0.0096 - val_mae: 0.1191 - learning_rate: 1.0000e-04\n",
      "Epoch 126/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0064 - mae: 0.1046 - val_loss: 0.0097 - val_mae: 0.1178 - learning_rate: 1.0000e-04\n",
      "Epoch 127/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0064 - mae: 0.1044 - val_loss: 0.0096 - val_mae: 0.1186 - learning_rate: 1.0000e-04\n",
      "Epoch 128/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0063 - mae: 0.1043 - val_loss: 0.0096 - val_mae: 0.1186 - learning_rate: 1.0000e-04\n",
      "Epoch 129/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0063 - mae: 0.1042 - val_loss: 0.0097 - val_mae: 0.1195 - learning_rate: 1.0000e-04\n",
      "Epoch 130/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0064 - mae: 0.1041 - val_loss: 0.0096 - val_mae: 0.1185 - learning_rate: 1.0000e-04\n",
      "Epoch 131/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0062 - mae: 0.1036 - val_loss: 0.0096 - val_mae: 0.1186 - learning_rate: 1.0000e-04\n",
      "Epoch 132/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0064 - mae: 0.1046 - val_loss: 0.0096 - val_mae: 0.1183 - learning_rate: 1.0000e-04\n",
      "Epoch 133/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0063 - mae: 0.1039 - val_loss: 0.0097 - val_mae: 0.1186 - learning_rate: 1.0000e-04\n",
      "Epoch 134/200\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 0.0063 - mae: 0.1037 - val_loss: 0.0097 - val_mae: 0.1191 - learning_rate: 1.0000e-04\n",
      "Val loss=0.00955, Val MAE=0.11712\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_length = 32\n",
    "df_pred = pd.DataFrame()\n",
    "features = ['ValoresNormalizados']\n",
    "\n",
    "for feature_column in features:\n",
    "    print(f\"Entrenando modo: {feature_column}\")\n",
    "\n",
    "    # Preparar los datos\n",
    "    X_train, X_test, y_train, y_test = split_data(df, context_length=32, feature=feature_column)\n",
    "    print(f\"Forma de X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"Forma de X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "    sample_weights = compute_sample_weights(y_train.flatten())\n",
    "\n",
    "    # Construir el modelo LSTM - biLSTM\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Conv1D(filters=64, kernel_size=5, padding='causal', activation='relu'),\n",
    "        Conv1D(filters=32, kernel_size=3, padding='causal', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        LSTM(64, return_sequences=True, kernel_regularizer=l2(0.0005)),\n",
    "        #Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.0005))),\n",
    "        Dropout(0.1),\n",
    "        LSTM(32, return_sequences=False, kernel_regularizer=l2(0.0005)),\n",
    "        #Bidirectional(LSTM(32, return_sequences=False, kernel_regularizer=l2(0.0005))),                                                                                                          \n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(0.0005)),\n",
    "        Dense(16, activation='relu', kernel_regularizer=l2(0.0005)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    def custom_loss(y_true, y_pred, alpha=0.6, beta=0.2, gamma=0.2):\n",
    "        \"\"\"\n",
    "        Loss personalizado para datos de precipitación que combina:\n",
    "        - Huber loss para robustez general\n",
    "        - Mayor peso a predicciones de lluvia alta (>0.6) - eventos importantes\n",
    "        - Mayor peso a predicciones de sequía (<0.1) - períodos secos críticos\n",
    "        \"\"\"\n",
    "        huber = tf.keras.losses.Huber(delta=1.5)(y_true, y_pred)\n",
    "        \n",
    "        # Peso extra para valores altos de precipitación (lluvia intensa)\n",
    "        high_precip_mask = tf.cast(y_true > 0.6, tf.float32)\n",
    "        high_weighted_component = tf.reduce_mean(high_precip_mask * tf.square(y_true - y_pred))\n",
    "        \n",
    "        # Peso extra para valores bajos de precipitación (sequía)\n",
    "        low_precip_mask = tf.cast(y_true < 0.1, tf.float32)\n",
    "        low_weighted_component = tf.reduce_mean(low_precip_mask * tf.square(y_true - y_pred))\n",
    "        \n",
    "        # Combinación balanceada: alpha*huber + beta*alta_lluvia + gamma*sequía\n",
    "        return alpha * huber + beta * high_weighted_component + gamma * low_weighted_component\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                loss=custom_loss,\n",
    "                metrics=['mae'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=30,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=200,\n",
    "        batch_size=1024,\n",
    "        validation_data=(X_test, y_test),\n",
    "        sample_weight=sample_weights,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Val loss={loss:.5f}, Val MAE={mae:.5f}\")\n",
    "\n",
    "    # Graficar historial de entrenamiento\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n",
    "    plt.plot(history.history['val_loss'], label='Pérdida de Validación')\n",
    "    plt.title(f'Historial de Entrenamiento - {feature_column}')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.join(DATA_FOLDER, \"img/models\"), exist_ok=True)\n",
    "    plt.savefig(os.path.join(DATA_FOLDER, f\"img/models/mlstm_{feature_column}.png\"), dpi=300)\n",
    "    #plt.savefig(os.path.join(DATA_FOLDER, f\"img/models/mbilstm_{feature_column}.png\"), dpi=300)\n",
    "\n",
    "    # Guardar el modelo \n",
    "    os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "    model.save(os.path.join(MODELS_FOLDER, f\"mlstm_{feature_column}.keras\"))\n",
    "    #model.save(os.path.join(MODELS_FOLDER, f\"mbilstm_{feature_column}.keras\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CEEMDAN VMD - biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modo: imf2\n",
      "Forma de X_train: (209570, 32, 1), y_train: (209570, 1)\n",
      "Forma de X_test: (54055, 32, 1), y_test: (54055, 1)\n",
      "Epoch 1/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 114ms/step - loss: 0.0824 - mae: 0.0850 - val_loss: 0.0094 - val_mae: 0.0630 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0076 - mae: 0.0591 - val_loss: 0.0058 - val_mae: 0.0545 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0057 - mae: 0.0546 - val_loss: 0.0051 - val_mae: 0.0511 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0051 - mae: 0.0521 - val_loss: 0.0046 - val_mae: 0.0490 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0048 - mae: 0.0507 - val_loss: 0.0043 - val_mae: 0.0472 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0045 - mae: 0.0490 - val_loss: 0.0042 - val_mae: 0.0465 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0043 - mae: 0.0482 - val_loss: 0.0041 - val_mae: 0.0460 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0042 - mae: 0.0473 - val_loss: 0.0040 - val_mae: 0.0453 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0041 - mae: 0.0465 - val_loss: 0.0040 - val_mae: 0.0458 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0040 - mae: 0.0463 - val_loss: 0.0039 - val_mae: 0.0448 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0039 - mae: 0.0457 - val_loss: 0.0037 - val_mae: 0.0437 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0039 - mae: 0.0453 - val_loss: 0.0037 - val_mae: 0.0433 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0039 - mae: 0.0457 - val_loss: 0.0036 - val_mae: 0.0431 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0037 - mae: 0.0445 - val_loss: 0.0035 - val_mae: 0.0429 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0038 - mae: 0.0447 - val_loss: 0.0038 - val_mae: 0.0435 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0037 - mae: 0.0445 - val_loss: 0.0035 - val_mae: 0.0429 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0036 - mae: 0.0435 - val_loss: 0.0034 - val_mae: 0.0418 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0036 - mae: 0.0435 - val_loss: 0.0033 - val_mae: 0.0413 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0035 - mae: 0.0431 - val_loss: 0.0032 - val_mae: 0.0399 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0035 - mae: 0.0430 - val_loss: 0.0031 - val_mae: 0.0398 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0034 - mae: 0.0422 - val_loss: 0.0031 - val_mae: 0.0392 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0034 - mae: 0.0420 - val_loss: 0.0032 - val_mae: 0.0405 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0032 - mae: 0.0408 - val_loss: 0.0030 - val_mae: 0.0386 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0033 - mae: 0.0416 - val_loss: 0.0028 - val_mae: 0.0372 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0032 - mae: 0.0402 - val_loss: 0.0030 - val_mae: 0.0386 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0031 - mae: 0.0395 - val_loss: 0.0031 - val_mae: 0.0395 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0029 - mae: 0.0383 - val_loss: 0.0025 - val_mae: 0.0342 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0029 - mae: 0.0379 - val_loss: 0.0025 - val_mae: 0.0343 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0028 - mae: 0.0374 - val_loss: 0.0026 - val_mae: 0.0354 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0028 - mae: 0.0375 - val_loss: 0.0026 - val_mae: 0.0352 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0028 - mae: 0.0374 - val_loss: 0.0025 - val_mae: 0.0347 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0028 - mae: 0.0377 - val_loss: 0.0024 - val_mae: 0.0337 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0027 - mae: 0.0367 - val_loss: 0.0025 - val_mae: 0.0353 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0027 - mae: 0.0364 - val_loss: 0.0024 - val_mae: 0.0338 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0026 - mae: 0.0358 - val_loss: 0.0025 - val_mae: 0.0346 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0026 - mae: 0.0359 - val_loss: 0.0023 - val_mae: 0.0337 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0027 - mae: 0.0365 - val_loss: 0.0023 - val_mae: 0.0328 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0025 - mae: 0.0351 - val_loss: 0.0023 - val_mae: 0.0332 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0026 - mae: 0.0362 - val_loss: 0.0023 - val_mae: 0.0330 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0025 - mae: 0.0352 - val_loss: 0.0023 - val_mae: 0.0326 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0025 - mae: 0.0351 - val_loss: 0.0022 - val_mae: 0.0321 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0025 - mae: 0.0355 - val_loss: 0.0021 - val_mae: 0.0313 - learning_rate: 0.0010\n",
      "Epoch 43/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0025 - mae: 0.0350 - val_loss: 0.0022 - val_mae: 0.0319 - learning_rate: 0.0010\n",
      "Epoch 44/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0024 - mae: 0.0348 - val_loss: 0.0020 - val_mae: 0.0307 - learning_rate: 0.0010\n",
      "Epoch 45/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0024 - mae: 0.0343 - val_loss: 0.0022 - val_mae: 0.0320 - learning_rate: 0.0010\n",
      "Epoch 46/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0023 - mae: 0.0339 - val_loss: 0.0023 - val_mae: 0.0321 - learning_rate: 0.0010\n",
      "Epoch 47/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0023 - mae: 0.0340 - val_loss: 0.0021 - val_mae: 0.0313 - learning_rate: 0.0010\n",
      "Epoch 48/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0024 - mae: 0.0344 - val_loss: 0.0022 - val_mae: 0.0323 - learning_rate: 0.0010\n",
      "Epoch 49/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0023 - mae: 0.0337 - val_loss: 0.0020 - val_mae: 0.0308 - learning_rate: 0.0010\n",
      "Epoch 50/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0023 - mae: 0.0338 - val_loss: 0.0020 - val_mae: 0.0317 - learning_rate: 0.0010\n",
      "Epoch 51/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0022 - mae: 0.0331 - val_loss: 0.0019 - val_mae: 0.0293 - learning_rate: 0.0010\n",
      "Epoch 52/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0023 - mae: 0.0333 - val_loss: 0.0019 - val_mae: 0.0298 - learning_rate: 0.0010\n",
      "Epoch 53/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0023 - mae: 0.0333 - val_loss: 0.0020 - val_mae: 0.0301 - learning_rate: 0.0010\n",
      "Epoch 54/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0022 - mae: 0.0330 - val_loss: 0.0018 - val_mae: 0.0285 - learning_rate: 0.0010\n",
      "Epoch 55/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0023 - mae: 0.0335 - val_loss: 0.0020 - val_mae: 0.0317 - learning_rate: 0.0010\n",
      "Epoch 56/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0022 - mae: 0.0334 - val_loss: 0.0021 - val_mae: 0.0316 - learning_rate: 0.0010\n",
      "Epoch 57/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0022 - mae: 0.0326 - val_loss: 0.0019 - val_mae: 0.0301 - learning_rate: 0.0010\n",
      "Epoch 58/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0021 - mae: 0.0322 - val_loss: 0.0018 - val_mae: 0.0292 - learning_rate: 0.0010\n",
      "Epoch 59/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0022 - mae: 0.0326 - val_loss: 0.0022 - val_mae: 0.0332 - learning_rate: 0.0010\n",
      "Epoch 60/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0022 - mae: 0.0326 - val_loss: 0.0018 - val_mae: 0.0288 - learning_rate: 0.0010\n",
      "Epoch 61/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0021 - mae: 0.0322 - val_loss: 0.0018 - val_mae: 0.0289 - learning_rate: 0.0010\n",
      "Epoch 62/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0021 - mae: 0.0324 - val_loss: 0.0020 - val_mae: 0.0301 - learning_rate: 0.0010\n",
      "Epoch 63/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0022 - mae: 0.0331 - val_loss: 0.0018 - val_mae: 0.0294 - learning_rate: 0.0010\n",
      "Epoch 64/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0021 - mae: 0.0321 - val_loss: 0.0018 - val_mae: 0.0286 - learning_rate: 0.0010\n",
      "Epoch 65/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0022 - mae: 0.0327 - val_loss: 0.0018 - val_mae: 0.0285 - learning_rate: 0.0010\n",
      "Epoch 66/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0024 - mae: 0.0345 - val_loss: 0.0017 - val_mae: 0.0284 - learning_rate: 0.0010\n",
      "Epoch 67/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0020 - mae: 0.0312 - val_loss: 0.0017 - val_mae: 0.0284 - learning_rate: 0.0010\n",
      "Epoch 68/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0026 - mae: 0.0352 - val_loss: 0.0026 - val_mae: 0.0360 - learning_rate: 0.0010\n",
      "Epoch 69/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0026 - mae: 0.0366 - val_loss: 0.0019 - val_mae: 0.0299 - learning_rate: 0.0010\n",
      "Epoch 70/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0022 - mae: 0.0328 - val_loss: 0.0021 - val_mae: 0.0319 - learning_rate: 0.0010\n",
      "Epoch 71/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0022 - mae: 0.0328 - val_loss: 0.0018 - val_mae: 0.0285 - learning_rate: 0.0010\n",
      "Epoch 72/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0021 - mae: 0.0316 - val_loss: 0.0017 - val_mae: 0.0276 - learning_rate: 0.0010\n",
      "Epoch 73/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0021 - mae: 0.0316 - val_loss: 0.0018 - val_mae: 0.0291 - learning_rate: 0.0010\n",
      "Epoch 74/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0021 - mae: 0.0316 - val_loss: 0.0020 - val_mae: 0.0301 - learning_rate: 0.0010\n",
      "Epoch 75/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0020 - mae: 0.0310 - val_loss: 0.0018 - val_mae: 0.0281 - learning_rate: 1.0000e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0019 - mae: 0.0300 - val_loss: 0.0017 - val_mae: 0.0278 - learning_rate: 1.0000e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0019 - mae: 0.0295 - val_loss: 0.0017 - val_mae: 0.0275 - learning_rate: 1.0000e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0018 - mae: 0.0292 - val_loss: 0.0016 - val_mae: 0.0272 - learning_rate: 1.0000e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0018 - mae: 0.0288 - val_loss: 0.0016 - val_mae: 0.0269 - learning_rate: 1.0000e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0018 - mae: 0.0290 - val_loss: 0.0016 - val_mae: 0.0265 - learning_rate: 1.0000e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0018 - mae: 0.0290 - val_loss: 0.0016 - val_mae: 0.0262 - learning_rate: 1.0000e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0018 - mae: 0.0288 - val_loss: 0.0015 - val_mae: 0.0257 - learning_rate: 1.0000e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0018 - mae: 0.0285 - val_loss: 0.0015 - val_mae: 0.0257 - learning_rate: 1.0000e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0017 - mae: 0.0284 - val_loss: 0.0015 - val_mae: 0.0263 - learning_rate: 1.0000e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0017 - mae: 0.0284 - val_loss: 0.0016 - val_mae: 0.0267 - learning_rate: 1.0000e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0018 - mae: 0.0289 - val_loss: 0.0015 - val_mae: 0.0262 - learning_rate: 1.0000e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0018 - mae: 0.0286 - val_loss: 0.0016 - val_mae: 0.0266 - learning_rate: 1.0000e-04\n",
      "Epoch 88/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0018 - mae: 0.0287 - val_loss: 0.0015 - val_mae: 0.0254 - learning_rate: 1.0000e-04\n",
      "Epoch 89/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0017 - mae: 0.0284 - val_loss: 0.0015 - val_mae: 0.0257 - learning_rate: 1.0000e-04\n",
      "Epoch 90/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0017 - mae: 0.0285 - val_loss: 0.0017 - val_mae: 0.0287 - learning_rate: 1.0000e-04\n",
      "Epoch 91/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0018 - mae: 0.0293 - val_loss: 0.0015 - val_mae: 0.0260 - learning_rate: 1.0000e-04\n",
      "Epoch 92/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0017 - mae: 0.0284 - val_loss: 0.0015 - val_mae: 0.0252 - learning_rate: 1.0000e-04\n",
      "Epoch 93/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0017 - mae: 0.0287 - val_loss: 0.0015 - val_mae: 0.0262 - learning_rate: 1.0000e-04\n",
      "Epoch 94/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0018 - mae: 0.0288 - val_loss: 0.0015 - val_mae: 0.0256 - learning_rate: 1.0000e-04\n",
      "Epoch 95/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0017 - mae: 0.0283 - val_loss: 0.0015 - val_mae: 0.0256 - learning_rate: 1.0000e-04\n",
      "Epoch 96/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0017 - mae: 0.0281 - val_loss: 0.0015 - val_mae: 0.0254 - learning_rate: 1.0000e-04\n",
      "Epoch 97/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0017 - mae: 0.0282 - val_loss: 0.0015 - val_mae: 0.0256 - learning_rate: 1.0000e-04\n",
      "Epoch 98/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0017 - mae: 0.0286 - val_loss: 0.0015 - val_mae: 0.0259 - learning_rate: 1.0000e-04\n",
      "Epoch 99/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0020 - mae: 0.0309 - val_loss: 0.0015 - val_mae: 0.0258 - learning_rate: 1.0000e-04\n",
      "Epoch 100/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0018 - mae: 0.0289 - val_loss: 0.0015 - val_mae: 0.0254 - learning_rate: 1.0000e-04\n",
      "Val loss=0.00146, Val MAE=0.02520\n",
      "Entrenando modo: imf3\n",
      "Forma de X_train: (209570, 32, 1), y_train: (209570, 1)\n",
      "Forma de X_test: (54055, 32, 1), y_test: (54055, 1)\n",
      "Epoch 1/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 113ms/step - loss: 0.0729 - mae: 0.0423 - val_loss: 0.0030 - val_mae: 0.0220 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0020 - mae: 0.0215 - val_loss: 0.0011 - val_mae: 0.0202 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0010 - mae: 0.0199 - val_loss: 9.6454e-04 - val_mae: 0.0193 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 8.8733e-04 - mae: 0.0189 - val_loss: 7.6664e-04 - val_mae: 0.0162 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 7.6512e-04 - mae: 0.0172 - val_loss: 7.9835e-04 - val_mae: 0.0180 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 7.1980e-04 - mae: 0.0170 - val_loss: 7.0538e-04 - val_mae: 0.0170 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 6.6988e-04 - mae: 0.0165 - val_loss: 6.6318e-04 - val_mae: 0.0163 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 7.1247e-04 - mae: 0.0173 - val_loss: 8.3468e-04 - val_mae: 0.0182 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 6.4123e-04 - mae: 0.0157 - val_loss: 5.2039e-04 - val_mae: 0.0129 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 6.4787e-04 - mae: 0.0163 - val_loss: 6.7365e-04 - val_mae: 0.0174 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 5.9502e-04 - mae: 0.0156 - val_loss: 0.0020 - val_mae: 0.0361 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0016 - mae: 0.0255 - val_loss: 4.9511e-04 - val_mae: 0.0124 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 5.4385e-04 - mae: 0.0141 - val_loss: 6.9847e-04 - val_mae: 0.0189 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 5.4458e-04 - mae: 0.0146 - val_loss: 4.7385e-04 - val_mae: 0.0125 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0017 - mae: 0.0232 - val_loss: 0.0011 - val_mae: 0.0163 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0011 - mae: 0.0193 - val_loss: 7.9731e-04 - val_mae: 0.0172 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 8.2675e-04 - mae: 0.0172 - val_loss: 5.8961e-04 - val_mae: 0.0130 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 6.8335e-04 - mae: 0.0156 - val_loss: 5.2403e-04 - val_mae: 0.0125 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 5.9657e-04 - mae: 0.0148 - val_loss: 4.8650e-04 - val_mae: 0.0127 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 5.4687e-04 - mae: 0.0143 - val_loss: 4.2455e-04 - val_mae: 0.0112 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 5.0803e-04 - mae: 0.0140 - val_loss: 4.0880e-04 - val_mae: 0.0110 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 4.8060e-04 - mae: 0.0136 - val_loss: 3.8119e-04 - val_mae: 0.0108 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 4.7877e-04 - mae: 0.0139 - val_loss: 3.8730e-04 - val_mae: 0.0107 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 4.5526e-04 - mae: 0.0134 - val_loss: 3.6258e-04 - val_mae: 0.0102 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 4.3327e-04 - mae: 0.0128 - val_loss: 3.2950e-04 - val_mae: 0.0098 - learning_rate: 0.0010\n",
      "Epoch 26/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 4.4270e-04 - mae: 0.0132 - val_loss: 3.5619e-04 - val_mae: 0.0108 - learning_rate: 0.0010\n",
      "Epoch 27/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 4.2990e-04 - mae: 0.0131 - val_loss: 3.4769e-04 - val_mae: 0.0107 - learning_rate: 0.0010\n",
      "Epoch 28/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 4.0872e-04 - mae: 0.0126 - val_loss: 3.4187e-04 - val_mae: 0.0108 - learning_rate: 0.0010\n",
      "Epoch 29/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 4.5369e-04 - mae: 0.0131 - val_loss: 0.0047 - val_mae: 0.0460 - learning_rate: 0.0010\n",
      "Epoch 30/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 0.0021 - mae: 0.0280 - val_loss: 9.6078e-04 - val_mae: 0.0188 - learning_rate: 0.0010\n",
      "Epoch 31/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 9.3791e-04 - mae: 0.0206 - val_loss: 7.1504e-04 - val_mae: 0.0164 - learning_rate: 0.0010\n",
      "Epoch 32/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 7.9039e-04 - mae: 0.0187 - val_loss: 6.0189e-04 - val_mae: 0.0146 - learning_rate: 0.0010\n",
      "Epoch 33/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 7.2903e-04 - mae: 0.0181 - val_loss: 5.5572e-04 - val_mae: 0.0143 - learning_rate: 0.0010\n",
      "Epoch 34/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 6.1514e-04 - mae: 0.0168 - val_loss: 6.7896e-04 - val_mae: 0.0191 - learning_rate: 0.0010\n",
      "Epoch 35/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 6.0834e-04 - mae: 0.0167 - val_loss: 5.0558e-04 - val_mae: 0.0141 - learning_rate: 0.0010\n",
      "Epoch 36/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 5.7736e-04 - mae: 0.0165 - val_loss: 4.7359e-04 - val_mae: 0.0134 - learning_rate: 0.0010\n",
      "Epoch 37/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 5.9462e-04 - mae: 0.0167 - val_loss: 4.3953e-04 - val_mae: 0.0126 - learning_rate: 0.0010\n",
      "Epoch 38/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 5.3918e-04 - mae: 0.0159 - val_loss: 5.0517e-04 - val_mae: 0.0153 - learning_rate: 0.0010\n",
      "Epoch 39/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 5.5226e-04 - mae: 0.0159 - val_loss: 4.4413e-04 - val_mae: 0.0136 - learning_rate: 0.0010\n",
      "Epoch 40/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 113ms/step - loss: 5.1730e-04 - mae: 0.0155 - val_loss: 4.2090e-04 - val_mae: 0.0126 - learning_rate: 0.0010\n",
      "Epoch 41/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 113ms/step - loss: 5.9916e-04 - mae: 0.0165 - val_loss: 6.8414e-04 - val_mae: 0.0170 - learning_rate: 0.0010\n",
      "Epoch 42/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 4.7291e-04 - mae: 0.0139 - val_loss: 3.9487e-04 - val_mae: 0.0119 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 3.9624e-04 - mae: 0.0126 - val_loss: 3.4161e-04 - val_mae: 0.0110 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 3.6747e-04 - mae: 0.0120 - val_loss: 3.1701e-04 - val_mae: 0.0105 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.4621e-04 - mae: 0.0116 - val_loss: 2.9489e-04 - val_mae: 0.0099 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 3.3471e-04 - mae: 0.0113 - val_loss: 2.7981e-04 - val_mae: 0.0094 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 3.2567e-04 - mae: 0.0111 - val_loss: 2.7255e-04 - val_mae: 0.0093 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 3.2227e-04 - mae: 0.0111 - val_loss: 2.6141e-04 - val_mae: 0.0090 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 3.1707e-04 - mae: 0.0110 - val_loss: 2.6809e-04 - val_mae: 0.0093 - learning_rate: 1.0000e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 3.0754e-04 - mae: 0.0107 - val_loss: 2.4999e-04 - val_mae: 0.0087 - learning_rate: 1.0000e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.0054e-04 - mae: 0.0107 - val_loss: 2.5438e-04 - val_mae: 0.0089 - learning_rate: 1.0000e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.9427e-04 - mae: 0.0105 - val_loss: 2.4889e-04 - val_mae: 0.0089 - learning_rate: 1.0000e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.8792e-04 - mae: 0.0104 - val_loss: 2.4092e-04 - val_mae: 0.0086 - learning_rate: 1.0000e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.8997e-04 - mae: 0.0105 - val_loss: 2.3314e-04 - val_mae: 0.0084 - learning_rate: 1.0000e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 2.8448e-04 - mae: 0.0105 - val_loss: 2.3987e-04 - val_mae: 0.0090 - learning_rate: 1.0000e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.7325e-04 - mae: 0.0102 - val_loss: 2.3496e-04 - val_mae: 0.0088 - learning_rate: 1.0000e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.7902e-04 - mae: 0.0104 - val_loss: 2.3178e-04 - val_mae: 0.0087 - learning_rate: 1.0000e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.6559e-04 - mae: 0.0101 - val_loss: 2.1960e-04 - val_mae: 0.0083 - learning_rate: 1.0000e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.6879e-04 - mae: 0.0103 - val_loss: 2.3737e-04 - val_mae: 0.0091 - learning_rate: 1.0000e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.6137e-04 - mae: 0.0101 - val_loss: 2.0990e-04 - val_mae: 0.0081 - learning_rate: 1.0000e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 2.5196e-04 - mae: 0.0099 - val_loss: 2.2501e-04 - val_mae: 0.0090 - learning_rate: 1.0000e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.5339e-04 - mae: 0.0100 - val_loss: 2.6535e-04 - val_mae: 0.0108 - learning_rate: 1.0000e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 2.5919e-04 - mae: 0.0102 - val_loss: 2.0935e-04 - val_mae: 0.0085 - learning_rate: 1.0000e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 2.5007e-04 - mae: 0.0100 - val_loss: 2.0289e-04 - val_mae: 0.0079 - learning_rate: 1.0000e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.4419e-04 - mae: 0.0098 - val_loss: 2.0215e-04 - val_mae: 0.0079 - learning_rate: 1.0000e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 2.3106e-04 - mae: 0.0093 - val_loss: 1.9405e-04 - val_mae: 0.0077 - learning_rate: 1.0000e-05\n",
      "Epoch 67/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.2895e-04 - mae: 0.0092 - val_loss: 1.9017e-04 - val_mae: 0.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 68/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.2924e-04 - mae: 0.0093 - val_loss: 1.8975e-04 - val_mae: 0.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 69/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.2788e-04 - mae: 0.0092 - val_loss: 1.9255e-04 - val_mae: 0.0077 - learning_rate: 1.0000e-05\n",
      "Epoch 70/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.2869e-04 - mae: 0.0093 - val_loss: 1.8965e-04 - val_mae: 0.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 71/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.2509e-04 - mae: 0.0092 - val_loss: 1.8757e-04 - val_mae: 0.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 72/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.2572e-04 - mae: 0.0092 - val_loss: 1.8803e-04 - val_mae: 0.0076 - learning_rate: 1.0000e-05\n",
      "Epoch 73/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.2660e-04 - mae: 0.0092 - val_loss: 1.8747e-04 - val_mae: 0.0075 - learning_rate: 1.0000e-05\n",
      "Epoch 74/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 2.2536e-04 - mae: 0.0092 - val_loss: 1.8585e-04 - val_mae: 0.0075 - learning_rate: 1.0000e-05\n",
      "Epoch 75/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.2517e-04 - mae: 0.0092 - val_loss: 1.8971e-04 - val_mae: 0.0078 - learning_rate: 1.0000e-05\n",
      "Epoch 76/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.2648e-04 - mae: 0.0092 - val_loss: 1.8503e-04 - val_mae: 0.0075 - learning_rate: 1.0000e-05\n",
      "Epoch 77/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.2213e-04 - mae: 0.0091 - val_loss: 1.8315e-04 - val_mae: 0.0074 - learning_rate: 1.0000e-05\n",
      "Epoch 78/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.2385e-04 - mae: 0.0092 - val_loss: 1.8088e-04 - val_mae: 0.0074 - learning_rate: 1.0000e-05\n",
      "Epoch 79/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 2.2374e-04 - mae: 0.0092 - val_loss: 1.8289e-04 - val_mae: 0.0074 - learning_rate: 1.0000e-05\n",
      "Epoch 80/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.2187e-04 - mae: 0.0092 - val_loss: 1.8221e-04 - val_mae: 0.0074 - learning_rate: 1.0000e-05\n",
      "Epoch 81/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.2225e-04 - mae: 0.0091 - val_loss: 1.8392e-04 - val_mae: 0.0075 - learning_rate: 1.0000e-05\n",
      "Epoch 82/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1860e-04 - mae: 0.0091 - val_loss: 1.7861e-04 - val_mae: 0.0073 - learning_rate: 1.0000e-05\n",
      "Epoch 83/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1804e-04 - mae: 0.0091 - val_loss: 1.8066e-04 - val_mae: 0.0074 - learning_rate: 1.0000e-05\n",
      "Epoch 84/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 2.1874e-04 - mae: 0.0091 - val_loss: 1.7990e-04 - val_mae: 0.0074 - learning_rate: 1.0000e-05\n",
      "Epoch 85/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 2.1806e-04 - mae: 0.0090 - val_loss: 1.7668e-04 - val_mae: 0.0073 - learning_rate: 1.0000e-05\n",
      "Epoch 86/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1643e-04 - mae: 0.0090 - val_loss: 1.7429e-04 - val_mae: 0.0072 - learning_rate: 1.0000e-05\n",
      "Epoch 87/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 2.1699e-04 - mae: 0.0090 - val_loss: 1.7573e-04 - val_mae: 0.0073 - learning_rate: 1.0000e-06\n",
      "Epoch 88/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1635e-04 - mae: 0.0090 - val_loss: 1.7639e-04 - val_mae: 0.0073 - learning_rate: 1.0000e-06\n",
      "Epoch 89/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1654e-04 - mae: 0.0090 - val_loss: 1.7567e-04 - val_mae: 0.0073 - learning_rate: 1.0000e-06\n",
      "Epoch 90/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1599e-04 - mae: 0.0090 - val_loss: 1.7518e-04 - val_mae: 0.0072 - learning_rate: 1.0000e-06\n",
      "Epoch 91/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1495e-04 - mae: 0.0090 - val_loss: 1.7511e-04 - val_mae: 0.0072 - learning_rate: 1.0000e-06\n",
      "Epoch 92/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1518e-04 - mae: 0.0090 - val_loss: 1.7491e-04 - val_mae: 0.0072 - learning_rate: 1.0000e-06\n",
      "Epoch 93/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1482e-04 - mae: 0.0090 - val_loss: 1.7529e-04 - val_mae: 0.0072 - learning_rate: 1.0000e-06\n",
      "Epoch 94/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1518e-04 - mae: 0.0090 - val_loss: 1.7532e-04 - val_mae: 0.0073 - learning_rate: 1.0000e-06\n",
      "Epoch 95/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1487e-04 - mae: 0.0090 - val_loss: 1.7488e-04 - val_mae: 0.0072 - learning_rate: 1.0000e-06\n",
      "Epoch 96/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1470e-04 - mae: 0.0090 - val_loss: 1.7520e-04 - val_mae: 0.0072 - learning_rate: 1.0000e-06\n",
      "Epoch 97/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 2.1482e-04 - mae: 0.0090 - val_loss: 1.7542e-04 - val_mae: 0.0073 - learning_rate: 1.0000e-06\n",
      "Epoch 98/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1473e-04 - mae: 0.0090 - val_loss: 1.7464e-04 - val_mae: 0.0072 - learning_rate: 1.0000e-06\n",
      "Epoch 99/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1409e-04 - mae: 0.0090 - val_loss: 1.7433e-04 - val_mae: 0.0072 - learning_rate: 1.0000e-06\n",
      "Epoch 100/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1606e-04 - mae: 0.0090 - val_loss: 1.7381e-04 - val_mae: 0.0072 - learning_rate: 1.0000e-06\n",
      "Val loss=0.00017, Val MAE=0.00721\n",
      "Entrenando modo: imf4\n",
      "Forma de X_train: (209570, 32, 1), y_train: (209570, 1)\n",
      "Forma de X_test: (54055, 32, 1), y_test: (54055, 1)\n",
      "Epoch 1/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 113ms/step - loss: 0.0705 - mae: 0.0223 - val_loss: 0.0019 - val_mae: 0.0096 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 0.0010 - mae: 0.0091 - val_loss: 2.3827e-04 - val_mae: 0.0077 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 1.9965e-04 - mae: 0.0081 - val_loss: 2.4561e-04 - val_mae: 0.0104 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 1.7601e-04 - mae: 0.0083 - val_loss: 1.3697e-04 - val_mae: 0.0066 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 1.3685e-04 - mae: 0.0072 - val_loss: 9.8192e-05 - val_mae: 0.0049 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 1.7262e-04 - mae: 0.0080 - val_loss: 1.0989e-04 - val_mae: 0.0057 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 1.2457e-04 - mae: 0.0067 - val_loss: 1.0361e-04 - val_mae: 0.0055 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 1.2433e-04 - mae: 0.0069 - val_loss: 1.0955e-04 - val_mae: 0.0060 - learning_rate: 0.0010\n",
      "Epoch 9/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 1.0877e-04 - mae: 0.0063 - val_loss: 9.2720e-05 - val_mae: 0.0053 - learning_rate: 0.0010\n",
      "Epoch 10/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 9.8944e-05 - mae: 0.0059 - val_loss: 7.2143e-05 - val_mae: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 11/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 9.3834e-05 - mae: 0.0058 - val_loss: 7.3133e-05 - val_mae: 0.0043 - learning_rate: 0.0010\n",
      "Epoch 12/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 1.0222e-04 - mae: 0.0062 - val_loss: 7.9975e-05 - val_mae: 0.0048 - learning_rate: 0.0010\n",
      "Epoch 13/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 8.8835e-05 - mae: 0.0055 - val_loss: 6.1520e-05 - val_mae: 0.0040 - learning_rate: 0.0010\n",
      "Epoch 14/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 9.1396e-05 - mae: 0.0058 - val_loss: 7.4257e-05 - val_mae: 0.0042 - learning_rate: 0.0010\n",
      "Epoch 15/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 9.7544e-05 - mae: 0.0058 - val_loss: 1.4240e-04 - val_mae: 0.0063 - learning_rate: 0.0010\n",
      "Epoch 16/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 1.0484e-04 - mae: 0.0054 - val_loss: 6.0800e-05 - val_mae: 0.0035 - learning_rate: 0.0010\n",
      "Epoch 17/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 8.1263e-05 - mae: 0.0053 - val_loss: 6.1415e-05 - val_mae: 0.0036 - learning_rate: 0.0010\n",
      "Epoch 18/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 7.4857e-05 - mae: 0.0048 - val_loss: 7.7730e-05 - val_mae: 0.0054 - learning_rate: 0.0010\n",
      "Epoch 19/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 7.8199e-05 - mae: 0.0051 - val_loss: 1.0254e-04 - val_mae: 0.0062 - learning_rate: 0.0010\n",
      "Epoch 20/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 8.2058e-05 - mae: 0.0054 - val_loss: 8.4770e-05 - val_mae: 0.0065 - learning_rate: 0.0010\n",
      "Epoch 21/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 7.4323e-05 - mae: 0.0051 - val_loss: 1.4641e-04 - val_mae: 0.0082 - learning_rate: 0.0010\n",
      "Epoch 22/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 8.8001e-05 - mae: 0.0057 - val_loss: 6.5500e-05 - val_mae: 0.0045 - learning_rate: 0.0010\n",
      "Epoch 23/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 6.5473e-05 - mae: 0.0047 - val_loss: 2.0028e-04 - val_mae: 0.0104 - learning_rate: 0.0010\n",
      "Epoch 24/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 2.1372e-04 - mae: 0.0087 - val_loss: 5.0743e-05 - val_mae: 0.0035 - learning_rate: 0.0010\n",
      "Epoch 25/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 5.7541e-05 - mae: 0.0039 - val_loss: 4.2881e-05 - val_mae: 0.0028 - learning_rate: 1.0000e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 5.2918e-05 - mae: 0.0037 - val_loss: 3.8248e-05 - val_mae: 0.0026 - learning_rate: 1.0000e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 5.1238e-05 - mae: 0.0036 - val_loss: 4.1530e-05 - val_mae: 0.0029 - learning_rate: 1.0000e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 4.8869e-05 - mae: 0.0036 - val_loss: 3.3166e-05 - val_mae: 0.0023 - learning_rate: 1.0000e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 4.7367e-05 - mae: 0.0036 - val_loss: 3.1020e-05 - val_mae: 0.0024 - learning_rate: 1.0000e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 4.6281e-05 - mae: 0.0037 - val_loss: 3.2013e-05 - val_mae: 0.0025 - learning_rate: 1.0000e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 4.4814e-05 - mae: 0.0036 - val_loss: 2.8797e-05 - val_mae: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 4.4324e-05 - mae: 0.0037 - val_loss: 2.8369e-05 - val_mae: 0.0025 - learning_rate: 1.0000e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 4.2689e-05 - mae: 0.0036 - val_loss: 2.8202e-05 - val_mae: 0.0024 - learning_rate: 1.0000e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 4.1494e-05 - mae: 0.0036 - val_loss: 2.6788e-05 - val_mae: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 4.1699e-05 - mae: 0.0036 - val_loss: 2.4904e-05 - val_mae: 0.0021 - learning_rate: 1.0000e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 4.0795e-05 - mae: 0.0036 - val_loss: 2.6378e-05 - val_mae: 0.0023 - learning_rate: 1.0000e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 4.0958e-05 - mae: 0.0037 - val_loss: 2.9107e-05 - val_mae: 0.0030 - learning_rate: 1.0000e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.9686e-05 - mae: 0.0036 - val_loss: 2.3954e-05 - val_mae: 0.0023 - learning_rate: 1.0000e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 4.0285e-05 - mae: 0.0036 - val_loss: 2.4124e-05 - val_mae: 0.0024 - learning_rate: 1.0000e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.9338e-05 - mae: 0.0036 - val_loss: 2.2550e-05 - val_mae: 0.0022 - learning_rate: 1.0000e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.8922e-05 - mae: 0.0036 - val_loss: 2.2093e-05 - val_mae: 0.0020 - learning_rate: 1.0000e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.8032e-05 - mae: 0.0036 - val_loss: 2.3697e-05 - val_mae: 0.0024 - learning_rate: 1.0000e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.8043e-05 - mae: 0.0036 - val_loss: 2.2139e-05 - val_mae: 0.0023 - learning_rate: 1.0000e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 112ms/step - loss: 3.8096e-05 - mae: 0.0036 - val_loss: 2.1956e-05 - val_mae: 0.0021 - learning_rate: 1.0000e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.6206e-05 - mae: 0.0035 - val_loss: 2.3324e-05 - val_mae: 0.0025 - learning_rate: 1.0000e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.8668e-05 - mae: 0.0037 - val_loss: 2.0143e-05 - val_mae: 0.0019 - learning_rate: 1.0000e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.7490e-05 - mae: 0.0036 - val_loss: 1.9586e-05 - val_mae: 0.0021 - learning_rate: 1.0000e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.6591e-05 - mae: 0.0035 - val_loss: 1.9015e-05 - val_mae: 0.0019 - learning_rate: 1.0000e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.3948e-05 - mae: 0.0033 - val_loss: 1.9219e-05 - val_mae: 0.0019 - learning_rate: 1.0000e-05\n",
      "Epoch 50/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.3801e-05 - mae: 0.0032 - val_loss: 1.8868e-05 - val_mae: 0.0019 - learning_rate: 1.0000e-05\n",
      "Epoch 51/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.3647e-05 - mae: 0.0032 - val_loss: 1.9019e-05 - val_mae: 0.0019 - learning_rate: 1.0000e-05\n",
      "Epoch 52/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.3244e-05 - mae: 0.0032 - val_loss: 1.8625e-05 - val_mae: 0.0018 - learning_rate: 1.0000e-05\n",
      "Epoch 53/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.3650e-05 - mae: 0.0032 - val_loss: 1.8340e-05 - val_mae: 0.0018 - learning_rate: 1.0000e-05\n",
      "Epoch 54/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.3050e-05 - mae: 0.0032 - val_loss: 1.8139e-05 - val_mae: 0.0017 - learning_rate: 1.0000e-05\n",
      "Epoch 55/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.2931e-05 - mae: 0.0032 - val_loss: 1.9136e-05 - val_mae: 0.0019 - learning_rate: 1.0000e-05\n",
      "Epoch 56/100\n",
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 111ms/step - loss: 3.3056e-05 - mae: 0.0032 - val_loss: 1.8547e-05 - val_mae: 0.0018 - learning_rate: 1.0000e-05\n",
      "Epoch 57/100\n",
      "\u001b[1m186/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m1s\u001b[0m 99ms/step - loss: 3.2979e-05 - mae: 0.0032"
     ]
    }
   ],
   "source": [
    "context_length = 32\n",
    "df_pred = pd.DataFrame()\n",
    "features = imf_c+vmd_c\n",
    "features = [f for f in features if f != 'imf1']\n",
    "\n",
    "for feature_column in features:\n",
    "    print(f\"Entrenando modo: {feature_column}\")\n",
    "\n",
    "    # Preparar los datos\n",
    "    X_train, X_test, y_train, y_test = split_data(df, context_length=32, feature=feature_column)\n",
    "    print(f\"Forma de X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"Forma de X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "    # Construir el modelo LSTM\n",
    "    model = Sequential([\n",
    "        Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "        Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l2(0.0005))),\n",
    "        Dropout(0.1),\n",
    "        Bidirectional(LSTM(32, return_sequences=False,kernel_regularizer=l2(0.0005))),\n",
    "        Dropout(0.3),\n",
    "        Dense(16, activation='swish', kernel_regularizer=l2(0.0005)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=50,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,\n",
    "        patience=20,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=100,\n",
    "        batch_size=1028,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Evaluar el modelo\n",
    "    loss, mae = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Val loss={loss:.5f}, Val MAE={mae:.5f}\")\n",
    "\n",
    "    # Graficar historial de entrenamiento\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(history.history['loss'], label='Pérdida de Entrenamiento')\n",
    "    plt.plot(history.history['val_loss'], label='Pérdida de Validación')\n",
    "    plt.title(f'Historial de Entrenamiento - {feature_column}')\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(os.path.join(DATA_FOLDER, \"img/models\"), exist_ok=True)\n",
    "    plt.savefig(os.path.join(DATA_FOLDER, f\"img/models/mbilstm_{feature_column}.png\"), dpi=300)\n",
    "\n",
    "    # Guardar el modelo \n",
    "    os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "    model.save(os.path.join(MODELS_FOLDER, f\"mbilstm_{feature_column}.keras\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train: (209570, 32, 1), y_train: (209570, 1)\n",
      "Forma de X_test: (54055, 32, 1), y_test: (54055, 1)\n",
      "Epoch 1/100 - loss: 0.6606 - mse: 0.0148 - svm_loss: 0.6458 - svm_acc: 0.6635 - lr: 0.001000 - val_loss: 0.0751\n",
      "\n",
      "Epoch 5: Error increased. Starting monitoring for 3 epochs.\n",
      "\n",
      "Epoch 7: Error rebounded more than 2x. Resetting LR to 0.001\n",
      "Epoch 11/100 - loss: 0.9718 - mse: 0.0089 - svm_loss: 0.9629 - svm_acc: 0.0684 - lr: 0.001000 - val_loss: 0.0433\n",
      "\n",
      "Epoch 12: Error increased. Starting monitoring for 3 epochs.\n",
      "Epoch 17: New best model saved with val_loss: 0.0380 and svm_acc: 0.1143\n",
      "Epoch 18: New best model saved with val_loss: 0.0378 and svm_acc: 0.1267\n",
      "\n",
      "Epoch 20: Error increased. Starting monitoring for 3 epochs.\n",
      "Epoch 21: New best model saved with val_loss: 0.0374 and svm_acc: 0.1230\n",
      "Epoch 21/100 - loss: 0.9521 - mse: 0.0083 - svm_loss: 0.9438 - svm_acc: 0.1230 - lr: 0.001000 - val_loss: 0.0374\n",
      "Epoch 31/100 - loss: 0.9304 - mse: 0.0079 - svm_loss: 0.9226 - svm_acc: 0.0757 - lr: 0.001000 - val_loss: 0.0385\n",
      "Epoch 32: New best model saved with val_loss: 0.0370 and svm_acc: 0.1276\n",
      "\n",
      "Epoch 34: Error increased. Starting monitoring for 3 epochs.\n",
      "Epoch 34: New best model saved with val_loss: 0.0366 and svm_acc: 0.1364\n",
      "\n",
      "Epoch 37: Error increased. Starting monitoring for 3 epochs.\n",
      "Epoch 41/100 - loss: 0.9180 - mse: 0.0075 - svm_loss: 0.9105 - svm_acc: 0.0839 - lr: 0.001000 - val_loss: 0.0368\n",
      "\n",
      "Epoch 45: Error increased. Starting monitoring for 3 epochs.\n",
      "Epoch 45: New best model saved with val_loss: 0.0357 and svm_acc: 0.1506\n",
      "\n",
      "Epoch 47: Error increased. Starting monitoring for 3 epochs.\n",
      "Epoch 51/100 - loss: 0.9123 - mse: 0.0071 - svm_loss: 0.9052 - svm_acc: 0.1135 - lr: 0.001000 - val_loss: 0.0372\n",
      "Epoch 61/100 - loss: 0.8948 - mse: 0.0068 - svm_loss: 0.8880 - svm_acc: 0.1149 - lr: 0.001000 - val_loss: 0.0373\n",
      "\n",
      "Epoch 66: Error increased. Starting monitoring for 3 epochs.\n",
      "\n",
      "Epoch 68: Error increased. Starting monitoring for 3 epochs.\n",
      "\n",
      "Epoch 71: Error increased. Starting monitoring for 3 epochs.\n",
      "Epoch 71/100 - loss: 0.8807 - mse: 0.0066 - svm_loss: 0.8742 - svm_acc: 0.1037 - lr: 0.001000 - val_loss: 0.0382\n",
      "\n",
      "Epoch 74: After 3 epochs of monitoring, error not decreasing.\n",
      "Reducing learning rate from 0.001000 to 0.000900\n",
      "\n",
      "Epoch 78: Error increased. Starting monitoring for 3 epochs.\n",
      "Epoch 81/100 - loss: 0.8996 - mse: 0.0063 - svm_loss: 0.8933 - svm_acc: 0.0966 - lr: 0.000900 - val_loss: 0.0389\n",
      "\n",
      "Epoch 85: Error increased. Starting monitoring for 3 epochs.\n",
      "\n",
      "Epoch 88: Error increased. Starting monitoring for 3 epochs.\n",
      "Epoch 91/100 - loss: 0.8590 - mse: 0.0061 - svm_loss: 0.8530 - svm_acc: 0.1231 - lr: 0.000900 - val_loss: 0.0379\n",
      "\n",
      "Epoch 92: Error increased. Starting monitoring for 3 epochs.\n",
      "\n",
      "Epoch 99: Error increased. Starting monitoring for 3 epochs.\n",
      "Epoch 100/100 - loss: 0.8658 - mse: 0.0059 - svm_loss: 0.8599 - svm_acc: 0.1174 - lr: 0.000900 - val_loss: 0.0385\n",
      "\n",
      "Test Metrics:\n",
      "MSE: 0.0248\n",
      "RMSE: 0.1574\n",
      "MAE: 0.1192\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Paths ===\n",
    "WORKSPACE = os.path.abspath(os.getcwd())\n",
    "DATA_FOLDER = os.path.join(WORKSPACE, 'productos/test/precipitation-forecast-co/data')\n",
    "MODELS_FOLDER = os.path.join(WORKSPACE, 'productos/test/precipitation-forecast-co/models')\n",
    "IMG_FOLDER = os.path.join(DATA_FOLDER, 'img', 'models')\n",
    "os.makedirs(IMG_FOLDER, exist_ok=True)\n",
    "os.makedirs(MODELS_FOLDER, exist_ok=True)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def custom_loss(y_true, y_pred, a=0.6, b=0.2, g=0.2):\n",
    "    h = tf.keras.losses.Huber(1.5)(y_true, y_pred)\n",
    "    hi = tf.reduce_mean(tf.cast(y_true > .6, tf.float32) * tf.square(y_true - y_pred))\n",
    "    lo = tf.reduce_mean(tf.cast(y_true < .1, tf.float32) * tf.square(y_true - y_pred))\n",
    "    return a*h + b*hi + g*lo\n",
    "\n",
    "def distributed_sample(y_data, num_samples=None):\n",
    "    y_data = y_data.flatten()\n",
    "    bins = [0.0, 0.2, 0.4, 0.6, 0.8, 1.01]\n",
    "    indices = []\n",
    "    per_bin = None\n",
    "    total_len = len(y_data)\n",
    "    if num_samples is not None:\n",
    "        per_bin = max(1, num_samples // 5)\n",
    "    for i in range(5):\n",
    "        bin_mask = (y_data >= bins[i]) & (y_data < bins[i+1])\n",
    "        bin_idx = np.where(bin_mask)[0]\n",
    "        if len(bin_idx) == 0:\n",
    "            continue\n",
    "        take = int(0.2 * total_len) if per_bin is None else min(per_bin, len(bin_idx))\n",
    "        chosen = np.random.choice(bin_idx, size=take, replace=len(bin_idx)<take)\n",
    "        indices.append(chosen)\n",
    "    return np.concatenate(indices)\n",
    "\n",
    "class SVM_Discriminator:\n",
    "    def __init__(self, kernel='rbf', probability=True):\n",
    "        self.svm = SVC(kernel=kernel, probability=True)\n",
    "        self.scaler = StandardScaler()\n",
    "    def fit(self, real_data, noise_data):\n",
    "        X = np.vstack([real_data.reshape(len(real_data), -1), noise_data.reshape(len(noise_data), -1)])\n",
    "        self.scaler.fit(X)\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        y = np.concatenate([np.ones(len(real_data)), np.zeros(len(noise_data))])\n",
    "        self.svm.fit(X_scaled, y)\n",
    "        return self\n",
    "    def predict_proba(self, data):\n",
    "        data_reshaped = data.reshape(len(data), -1)\n",
    "        data_scaled = self.scaler.transform(data_reshaped)\n",
    "        return self.svm.predict_proba(data_scaled)[:, 1]\n",
    "\n",
    "def build_lstm_model(input_shape):\n",
    "    return Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv1D(64, 5, padding='causal', activation='relu'),\n",
    "        Conv1D(32, 3, padding='causal', activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        LSTM(64, return_sequences=True, kernel_regularizer=l2(5e-4)),\n",
    "        Dropout(0.1),\n",
    "        LSTM(32, kernel_regularizer=l2(5e-4)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu', kernel_regularizer=l2(5e-4)),\n",
    "        Dense(16, activation='relu', kernel_regularizer=l2(5e-4)),\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "class SBiLSTM_Model:\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "        self.lstm = build_lstm_model(input_shape)\n",
    "        self.discriminator = SVM_Discriminator()\n",
    "        self.initial_lr = 0.001\n",
    "        self.optimizer = Adam(learning_rate=self.initial_lr, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "        self.best_loss = float('inf')\n",
    "        self.monitor_active = False\n",
    "        self.monitor_count = 0\n",
    "        self.monitor_start_loss = 0\n",
    "        self.cooldown_counter = 0\n",
    "        self.monitor_period = 3\n",
    "        self.cooldown_period = 3\n",
    "\n",
    "    def _compute_sample_weights(self, y_data):\n",
    "        mean_value = np.mean(y_data)\n",
    "        deviations = np.abs(y_data - mean_value)\n",
    "        max_dev = np.max(deviations) + 1e-10\n",
    "        weights = 0.5 + 2.5 * (deviations / max_dev)\n",
    "        return weights\n",
    "\n",
    "    def _adjust_learning_rate(self, current_loss, epoch):\n",
    "        current_lr = self.optimizer.learning_rate.numpy()\n",
    "        if self.cooldown_counter > 0:\n",
    "            self.cooldown_counter -= 1\n",
    "            return\n",
    "        if current_loss < self.best_loss:\n",
    "            self.best_loss = current_loss\n",
    "            if self.monitor_active:\n",
    "                self.monitor_active = False\n",
    "        elif current_loss > self.best_loss:\n",
    "            if current_loss > 2 * self.best_loss:\n",
    "                print(f\"\\nEpoch {epoch+1}: Error rebounded more than 2x. Resetting LR to 0.001\")\n",
    "                self.optimizer.learning_rate.assign(0.001)\n",
    "                self.monitor_active = False\n",
    "                self.best_loss = current_loss\n",
    "                return\n",
    "            if not self.monitor_active:\n",
    "                self.monitor_active = True\n",
    "                self.monitor_count = 0\n",
    "                self.monitor_start_loss = current_loss\n",
    "                print(f\"\\nEpoch {epoch+1}: Error increased. Starting monitoring for {self.monitor_period} epochs.\")\n",
    "            else:\n",
    "                self.monitor_count += 1\n",
    "                if self.monitor_count >= self.monitor_period:\n",
    "                    if current_loss >= self.monitor_start_loss:\n",
    "                        new_lr = current_lr * 0.9\n",
    "                        self.optimizer.learning_rate.assign(new_lr)\n",
    "                        print(f\"\\nEpoch {epoch+1}: After {self.monitor_period} epochs of monitoring, error not decreasing.\")\n",
    "                        print(f\"Reducing learning rate from {current_lr:.6f} to {new_lr:.6f}\")\n",
    "                    self.monitor_active = False\n",
    "                    self.cooldown_counter = self.cooldown_period\n",
    "\n",
    "    def train(self, X_train, y_train, noise_data, epochs=100, batch_size=1024, validation_data=None, save_path=None):\n",
    "        history = {'loss': [], 'mse_loss': [], 'svm_loss': [], 'svm_accuracy': [], 'learning_rate': []}\n",
    "        if validation_data is not None:\n",
    "            history['val_loss'] = []\n",
    "            X_val, y_val = validation_data\n",
    "        sample_weights = self._compute_sample_weights(y_train)\n",
    "        max_samples = 10000\n",
    "        n_samples = min(len(y_train), max_samples)\n",
    "        idx = distributed_sample(y_train, num_samples=n_samples)\n",
    "        noise_data_sample = noise_data[idx]\n",
    "        y_train_sample = y_train[idx]\n",
    "        self.discriminator.fit(y_train_sample, noise_data_sample)\n",
    "        best_val_loss = float('inf')\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            epoch_mse = 0\n",
    "            epoch_svm = 0\n",
    "            indices = np.random.permutation(len(X_train))\n",
    "            for i in range(0, len(X_train), batch_size):\n",
    "                batch_indices = indices[i:min(i+batch_size, len(X_train))]\n",
    "                X_batch = X_train[batch_indices]\n",
    "                y_batch = y_train[batch_indices]\n",
    "                weights_batch = sample_weights[batch_indices]\n",
    "                with tf.GradientTape() as tape:\n",
    "                    predictions = self.lstm(X_batch, training=True)\n",
    "                    mse_loss = custom_loss(tf.convert_to_tensor(y_batch, dtype=tf.float32), predictions)\n",
    "                    svm_probs = self.discriminator.predict_proba(predictions.numpy())\n",
    "                    svm_probs = tf.convert_to_tensor(svm_probs, dtype=tf.float32)\n",
    "                    svm_loss = -tf.reduce_mean(tf.math.log(svm_probs + 1e-10))\n",
    "                    total_loss = mse_loss + svm_loss\n",
    "                gradients = tape.gradient(total_loss, self.lstm.trainable_variables)\n",
    "                self.optimizer.apply_gradients(zip(gradients, self.lstm.trainable_variables))\n",
    "                epoch_loss += total_loss.numpy()\n",
    "                epoch_mse += mse_loss.numpy()\n",
    "                epoch_svm += svm_loss.numpy()\n",
    "            num_batches = np.ceil(len(X_train) / batch_size)\n",
    "            epoch_loss /= num_batches\n",
    "            epoch_mse /= num_batches\n",
    "            epoch_svm /= num_batches\n",
    "            self._adjust_learning_rate(epoch_loss, epoch)\n",
    "            current_lr = self.optimizer.learning_rate.numpy()\n",
    "            train_preds = self.lstm.predict(X_train, verbose=0)\n",
    "            svm_scores = self.discriminator.predict_proba(train_preds)\n",
    "            svm_accuracy = np.mean(svm_scores > 0.5)\n",
    "            history['loss'].append(epoch_loss)\n",
    "            history['mse_loss'].append(epoch_mse)\n",
    "            history['svm_loss'].append(epoch_svm)\n",
    "            history['svm_accuracy'].append(svm_accuracy)\n",
    "            history['learning_rate'].append(current_lr)\n",
    "            if validation_data is not None:\n",
    "                val_preds = self.lstm.predict(X_val, verbose=0)\n",
    "                val_weights = self._compute_sample_weights(y_val)\n",
    "                val_mse = np.mean((val_preds.flatten() - y_val.flatten()) ** 2 * val_weights.flatten())\n",
    "                history['val_loss'].append(val_mse)\n",
    "                if save_path and val_mse < best_val_loss and 0.1 <= svm_accuracy <= 0.3:\n",
    "                    best_val_loss = val_mse\n",
    "                    self.lstm.save(save_path)\n",
    "                    print(f\"Epoch {epoch+1}: New best model saved with val_loss: {val_mse:.4f} and svm_acc: {svm_accuracy:.4f}\")\n",
    "            if epoch % 5 == 0:\n",
    "                train_preds = self.lstm.predict(X_train, verbose=0)\n",
    "                real_data = y_train\n",
    "                fake_data = train_preds\n",
    "                n_samples = min(len(real_data), len(fake_data), max_samples)\n",
    "                idx = distributed_sample(real_data, num_samples=n_samples)\n",
    "                real_data_sample = real_data[idx]\n",
    "                fake_data_sample = fake_data[idx]\n",
    "                self.discriminator.fit(real_data_sample, fake_data_sample)\n",
    "            if epoch % 10 == 0 or epoch == epochs - 1:\n",
    "                log_msg = f\"Epoch {epoch+1}/{epochs} - loss: {epoch_loss:.4f} - mse: {epoch_mse:.4f} - svm_loss: {epoch_svm:.4f} - svm_acc: {svm_accuracy:.4f} - lr: {current_lr:.6f}\"\n",
    "                if validation_data is not None:\n",
    "                    log_msg += f\" - val_loss: {history['val_loss'][-1]:.4f}\"\n",
    "                print(log_msg)\n",
    "        if save_path and validation_data is None:\n",
    "            self.lstm.save(save_path)\n",
    "            print(f\"Final model saved to {save_path}\")\n",
    "        return history\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.lstm.predict(X, verbose=0)\n",
    "\n",
    "def apply_sbilstm_methodology(df, lag_steps=32):\n",
    "    assert all(isinstance(x, np.ndarray) for x in df['ValoresNormalizados'])\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "    for window in df['ValoresNormalizados']:\n",
    "        X, y = [], []\n",
    "        for i in range(len(window) - lag_steps):\n",
    "            X.append(window[i:i+lag_steps])\n",
    "            y.append(window[i+lag_steps])\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).reshape(-1, 1)\n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train.append(X[:split_idx])\n",
    "        X_test.append(X[split_idx:])\n",
    "        y_train.append(y[:split_idx])\n",
    "        y_test.append(y[split_idx:])\n",
    "    X_train = np.concatenate(X_train, axis=0)\n",
    "    X_test = np.concatenate(X_test, axis=0)\n",
    "    y_train = np.concatenate(y_train, axis=0)\n",
    "    y_test = np.concatenate(y_test, axis=0)\n",
    "    X_train = X_train.reshape(-1, lag_steps, 1)\n",
    "    X_test = X_test.reshape(-1, lag_steps, 1)\n",
    "    print(f\"Forma de X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "    print(f\"Forma de X_test: {X_test.shape}, y_test: {y_test.shape}\")\n",
    "    y_train = y_train.astype(np.float32)\n",
    "    mean = np.mean(y_train)\n",
    "    std = np.std(y_train) or 0.05\n",
    "    noise_data = np.clip(np.random.normal(mean, std * 0.1, y_train.shape), 0, 1)\n",
    "    model_path = os.path.join(MODELS_FOLDER, 'msbilstm_ValoresNormalizados.keras')\n",
    "    model = SBiLSTM_Model((lag_steps, 1))\n",
    "    history = model.train(X_train, y_train, noise_data=noise_data, epochs=100, batch_size=1024, validation_data=(X_test, y_test), save_path=model_path)\n",
    "    predictions = model.predict(X_test)\n",
    "    mse = np.mean((predictions.flatten() - y_test.flatten()) ** 2)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = np.mean(np.abs(predictions.flatten() - y_test.flatten()))\n",
    "    print(f\"\\nTest Metrics:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(history['mse_loss'], label='MSE Loss')\n",
    "    plt.plot(history['svm_loss'], label='SVM Loss')\n",
    "    plt.plot(history['loss'], label='Total Loss')\n",
    "    if 'val_loss' in history and history['val_loss']:\n",
    "        plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Training Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(IMG_FOLDER, 'msbilstm_ValoresNormalizados.png'))\n",
    "    plt.close()\n",
    "    return model, history, predictions\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    processing_data = os.path.join(DATA_FOLDER, 'processing/processing_month.parquet')\n",
    "    df = pd.read_parquet(processing_data)\n",
    "    model, history, predictions = apply_sbilstm_methodology(df, lag_steps=32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
